%!TEX root = ../../main.tex

\chapter{Evaluation}
\section{Predicting the winner}
In this phase of the CRISP-DM Process we are going to evaluate the models and the features, we previously generated.\newline
The evaluation criteria for the models and in the same way for the features which are used, is based on the test-accuracy. The highest accuracy we reached jet, have been 56.25\% with the Sliding Window Option 1, which means with the highest amount of features. The more features we have the less training samples are there for our training, so it could be better to train with more samples and less features. But because of the reason that every models are in the same range, which means there is no model which has a much smaller accuracy, it is not possible to tell the one with the highest accuracy the best model or has the best feature selection in every case. If you are using different data or a higher or smaller amount of features it could be possible to reach a better accuracy with another model than the one which reached the highest accuracy in the actual case. The first ranked model is using Keras Sequantial Neural Network, as shown in \autoref{table:coparison_of_classifiers}:

\begin{table}[H]
\centering
\begin{tabular}{|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
\hline

\textbf{Rank} & \textbf{Model} & \textbf{Sliding Window Option} & \textbf{Parameters} & \textbf{Test-Accuracy} \\ \hline
\textbf{1} & Keras Sequential Neural Network & 3 & Hidden layers: 2 (21, 21 neurons) & 0.5625 \\ \hline
\textbf{2} & Multi-layer Perceptron & 1 & Hidden layers: 2 (52, 32 neurons) & 0.5345 \\ \hline
\textbf{3} & Decision Tree & 1 & Depth = 4 & 0.5295 \\ \hline

\end{tabular}
\caption{Comparison of classifiers}
\label{table:coparison_of_classifiers}
\end{table}

As you can see in the table \autoref{table:coparison_of_classifiers}, too, you can not even tell, that the more features you use, the better the outcome will be. For example with the Multi-layer Perceptron you get a worse accuracy with more features. But for the actual situation we recommend to use the first ranked model for predicting the outcome for football games, but we will maybe use a different model with other features in the future.

\section{Predicting the final goals}
\subsection{Quality Criteria}
--> TODO Khaled
\subsection{Comparing the regressors}
This section highlights the different regressor models created for two different datasets with different features and then these models are compared to find the best model to make predictions on the new data or test data, we have created three different models using tried trial and error to set parameters and hiddenlayers units.\newline 
In classification tasks, it is easy to calculate sensitivity or specificity of classifier because output is always binary {correct classification, incorrect classification}. So we can count good/bad answers and based on the confusion matrix calculate some measurements. But in regression tasks, the output is a number. So we can't just say is it correct or incorrect but instead we should measure "how far from true solution we are" by either calculating coefiicient of determination Rsquare or by focusing on minimizing the mean squared error also known as loss. That is why in our regression models, we have calculated loss for validation as well as for training.
For all the three models, input units have been taken equal to features that is 21 units and as we are predicting two outputs- Home Team Goals and Away Team Goals, the output layers are having 2 units or neurons. For the dataset sliding02, we have used 3 hidden layers from which first hidden layer has 30 units which makes the neural network densely connected with 20 units in the second hidden layer and 10 in the third hidden layer. While training the model, we have observed the validation accuracy of 68 percent and Training accuracy of 70 percent. For second model, we have used three hidden layers with 21 units in first hidden layer making the artificial neural network as fullyconnected, 10 units in second hidden layer and 5 in third hidden layer which give us validation accuracy of 66 percent and training accuracy of 72 percent. And the third model have 4 hidden layers with 21 units in first hidden layer layer, 14 units in second hidden layer, 12 units in third hidden layer and 10 units in last hidden layer which give us validation accuracy of 56 percent and training accuracy of 69 percent.\newline
Similarly, similar experiments have been made with dataset sliding03 as well. For all the models we have also evaluated the quality for each team and for both training dataset and test dataset using the quality criteria defined in the above section. The evaluated qualities for all three models for dataset sliding02 and sliding03 have displayed in the respective tables \autoref{table:qualitymodelregression1} and \autoref{table:qualitymodelregression2} where QM is abbreviation of Quality Model, HG is abbreviation of Home goals and AG is abbreviation of Away goals
.\newline

\begin{table}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline

\textbf{Dataset
Sliding02
Models
} & \textbf{QM-HG-Train} & \textbf{QM-AG-Train} & \textbf{FinalQM- Train} & \textbf{QM-HG-Test} & \textbf{ QM-AG-Test } & \textbf{ FinalQM- Test} \\ \hline
\textbf{ Model1 } & 0.83 & 0.84 & 0.83 & 0.81 & 0.83 & 0.82 \\ \hline
\textbf{ Model2 } & 0.82 & 0.84 & 0.83 & 0.81 & 0.84 & 0.83 \\ \hline
\textbf{ Model3 } & 0.82 & 0.84 & 0.83 & 0.81 & 0.84 & 0.83 \\ \hline
\end{tabular}
}
\caption{ Quality Model for models with different hidden units for dataset sliding02}
\label{table:qualitymodelregression1}
\end{table}



\begin{table}
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline

\textbf{Dataset
Sliding03
Models
} & \textbf{QM-HG-Train} & \textbf{QM-AG-Train} & \textbf{FinalQM- Train} & \textbf{QM-HG-Test} & \textbf{ QM-AG-Test } & \textbf{ FinalQM- Test} \\ \hline
\textbf{ Model1 } & 0.83 & 0.84 & 0.84 & 0.81 & 0.83 & 0.82 \\ \hline
\textbf{ Model2 } & 0.80 & 0.83 & 0.82 & 0.80 & 0.84 & 0.82 \\ \hline
\textbf{ Model3 } & 0.83 & 0.84 & 0.83 & 0.81 & 0.84 & 0.82 \\ \hline
\end{tabular}
}
\caption{ Quality Model for models with different hidden units for dataset sliding03}
\label{table:qualitymodelregression2}
\end{table}

We have used first model from dataset sliding02 as our best model as the  validation and training accuracy of this model is better than the other models with a percentage of 68 percent and 72 percent respectively,ideally the validation accuracy must be slightly better than the training accuracy to prevent overfitting of the model. Because if the training loss is higher than validation loss then there are chances of overfitting and if the training loss is very less than validation loss then there are chances of underfitting.But still the validation accuracy is quite lower than the training accuracy.\newline
Also, as our approach is to minimize the mse that is mean squared error in order to get predictions on test data as close to actual data, this model provides us with minimum training mse of 0.19 and validation mse of 0.21. Hence, we are considering it as our best model.\newline
Using quality criteria defined, we have observed a final quality of performance for model 1 for Training data is 83 percent and for test data it is 82 percent.One important point is that the quality evaluation for all the models are showing almost similar values to each other which is not an ideal solution, so there are lot of scope of impovement in the project.\newline

\subsection{Comparing Regression with Multi Class Classification}
As it is described before the Multi Class Classification has still many possibilities to improve, so in this point we only compare the actual model with the best regression model. For this we will use our own Quality Criteria, but as it is described in the part `Quality Criteria', this has some disadvantages and has to be improved in the future. Because of this, the test accuracy is also used for the comparison. For the Multi Class Classification approach we are reaching a test accuracy of 30.54\% for the away team and 37.64\% for the home team, which makes an average of 34.09\%. With the regression model we are reaching a test accuracy of [HEMI U HAVE TO FILL IN]. Using our quality criteria we are reaching and average of  80.38\% (77.30\% for the home team and 83.47\% for the away team) with the Multi Class Classification and [HEMI U HAVE TO FILL IN] with the regression approach. Because of the better results with the regression model the team decided to use this model for the homepage. After improving the regression and the Multi Class Classification this could change. 