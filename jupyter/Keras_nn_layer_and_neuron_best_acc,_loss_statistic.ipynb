{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thu-soccer/project/blob/master/colab/colab_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "FtJth4hT577a",
    "outputId": "ce296e87-fcd1-4261-8057-6daacd747370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18740 train examples\n",
      "2083 test examples\n",
      "6329 train examples\n",
      "704 test examples\n",
      "6329 train examples\n",
      "704 test examples\n",
      "6296 train examples\n",
      "700 test examples\n",
      "6296 train examples\n",
      "700 test examples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_core.estimator import inputs\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def normalize_and_encode(dataframe):\n",
    "    column_names_to_not_normalize = ['result']\n",
    "    column_names_to_normalize = [x for x in list(dataframe) if x not in column_names_to_not_normalize ]\n",
    "    x = dataframe[column_names_to_normalize].values\n",
    "    x_scaled = preprocessing.normalize(x)\n",
    "    df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = dataframe.index)\n",
    "    dataframe[column_names_to_normalize] = df_temp\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit([ \"H\", \"A\", \"D\"])\n",
    "    dataframe.loc[:,['result']]=le.transform(dataframe['result'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def get_X_and_y(dataframe):\n",
    "    X = dataframe.drop(columns=['result']).values\n",
    "    y = dataframe[['result']].values\n",
    "    return X,y\n",
    "\n",
    "df01 = pd.read_csv('../data/sliding01.csv', sep=',', index_col=0)\n",
    "df02 = pd.read_csv('../data/sliding02_shots.csv', sep=',', index_col=0)\n",
    "df03 = pd.read_csv('../data/sliding03_shots_extra.csv', sep=',', index_col=0)\n",
    "df04 = pd.read_csv('../data/sliding04_shots_and_possession.csv', sep=',', index_col=0)\n",
    "df05 = pd.read_csv('../data/sliding05_shots_and_possession_extra.csv', sep=',', index_col=0)\n",
    "\n",
    "n01 = normalize_and_encode(df01)\n",
    "n02 = normalize_and_encode(df02)\n",
    "n03 = normalize_and_encode(df03)\n",
    "n04 = normalize_and_encode(df04)\n",
    "n05 = normalize_and_encode(df05)\n",
    "\n",
    "train01, test01 = train_test_split(n01, test_size=0.1)\n",
    "print(len(train01), 'train examples')\n",
    "print(len(test01), 'test examples')\n",
    "\n",
    "train02, test02 = train_test_split(n02, test_size=0.1)\n",
    "print(len(train02), 'train examples')\n",
    "print(len(test02), 'test examples')\n",
    "\n",
    "train03, test03 = train_test_split(n03, test_size=0.1)\n",
    "print(len(train03), 'train examples')\n",
    "print(len(test03), 'test examples')\n",
    "\n",
    "train04, test04 = train_test_split(n04, test_size=0.1)\n",
    "print(len(train04), 'train examples')\n",
    "print(len(test04), 'test examples')\n",
    "\n",
    "train05, test05 = train_test_split(n05, test_size=0.1)\n",
    "print(len(train04), 'train examples')\n",
    "print(len(test04), 'test examples')\n",
    "\n",
    "train_X01,train_y01 = get_X_and_y(train01)\n",
    "train_X02,train_y02 = get_X_and_y(train02)\n",
    "train_X03,train_y03 = get_X_and_y(train03)\n",
    "train_X04,train_y04 = get_X_and_y(train04)\n",
    "train_X05,train_y05 = get_X_and_y(train05)\n",
    "\n",
    "test_X01,test_y01 = get_X_and_y(test01)\n",
    "test_X02,test_y02 = get_X_and_y(test02)\n",
    "test_X03,test_y03 = get_X_and_y(test03)\n",
    "test_X04,test_y04 = get_X_and_y(test04)\n",
    "test_X05,test_y05 = get_X_and_y(test05)\n",
    "\n",
    "\n",
    "#Many models train better if you gradually reduce the learning rate during training. Use optimizers.schedules to reduce the learning rate over time:\n",
    "#The code sets a schedules.InverseTimeDecay to hyperbolically decrease the learning rate to 1/2 of the base rate at 1000 epochs, 1/3 at 2000 epochs and so on.\n",
    "\n",
    "def get_lr_schedule(train, batch_size):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=(len(train)//batch_size)*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "    return lr_schedule\n",
    "\n",
    "def get_optimizer(train, batch_size):\n",
    "    return tf.keras.optimizers.Adam(get_lr_schedule(train, batch_size))\n",
    "\n",
    "\n",
    "#Each model in this tutorial will use the same training configuration. So set these up in a reusable way, starting with the list of callbacks.\n",
    "#The training for this tutorial runs for many short epochs. To reduce the logging noise use the tfdocs.EpochDots which simply a . for each epoch and, and a full set of metrics every 100 epochs.\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200),\n",
    "        #tf.keras.callbacks.TensorBoard(logdir/name), # Jupyter Notebook\n",
    "        #tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1) # Google Colab\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, name, X, y, validation_split, batch_size, optimizer=None, max_epochs=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer(X, batch_size)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy','mse', 'mae', 'mape'])\n",
    "\n",
    "    model.summary()\n",
    "     \n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=validation_split,\n",
    "        batch_size=batch_size,\n",
    "#        steps_per_epoch = 50, # (len(train_X01)//batch_size,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=get_callbacks(name),\n",
    "        verbose=0)\n",
    "    \n",
    "    model.save(\"../model/%s.h5\" %name) \n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_history(model_history):\n",
    "\tplt.plot(model_history.history['accuracy'])\n",
    "\tplt.plot(model_history.history['val_accuracy'])\n",
    "\tplt.title(\"%s accuracy\" %model_history)\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n",
    "\t\n",
    "\tplt.plot(model_history.history['loss'])\n",
    "\tplt.plot(model_history.history['val_loss'])\n",
    "\tplt.title(\"%s loss\" %model_history)\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "C5oCSP7gaQ9y",
    "outputId": "6e0a4bc2-e646-4474-bad8-bd213594e680"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>odds-home</th>\n",
       "      <th>odds-draw</th>\n",
       "      <th>odds-away</th>\n",
       "      <th>home-wins</th>\n",
       "      <th>home-draws</th>\n",
       "      <th>home-losses</th>\n",
       "      <th>home-goals</th>\n",
       "      <th>home-opposition-goals</th>\n",
       "      <th>away-wins</th>\n",
       "      <th>away-draws</th>\n",
       "      <th>away-losses</th>\n",
       "      <th>away-goals</th>\n",
       "      <th>away-opposition-goals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.049957</td>\n",
       "      <td>0.165301</td>\n",
       "      <td>0.330601</td>\n",
       "      <td>0.183667</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.073467</td>\n",
       "      <td>0.514268</td>\n",
       "      <td>0.367334</td>\n",
       "      <td>0.073467</td>\n",
       "      <td>0.073467</td>\n",
       "      <td>0.220401</td>\n",
       "      <td>0.293868</td>\n",
       "      <td>0.514268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.077897</td>\n",
       "      <td>0.103862</td>\n",
       "      <td>0.121172</td>\n",
       "      <td>0.103862</td>\n",
       "      <td>0.138483</td>\n",
       "      <td>0.103862</td>\n",
       "      <td>0.553931</td>\n",
       "      <td>0.415448</td>\n",
       "      <td>0.138483</td>\n",
       "      <td>0.103862</td>\n",
       "      <td>0.103862</td>\n",
       "      <td>0.484690</td>\n",
       "      <td>0.415448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.109311</td>\n",
       "      <td>0.117119</td>\n",
       "      <td>0.105407</td>\n",
       "      <td>0.195198</td>\n",
       "      <td>0.078079</td>\n",
       "      <td>0.117119</td>\n",
       "      <td>0.390396</td>\n",
       "      <td>0.312317</td>\n",
       "      <td>0.156158</td>\n",
       "      <td>0.156158</td>\n",
       "      <td>0.078079</td>\n",
       "      <td>0.585594</td>\n",
       "      <td>0.507514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.068789</td>\n",
       "      <td>0.117049</td>\n",
       "      <td>0.155945</td>\n",
       "      <td>0.180075</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>0.072030</td>\n",
       "      <td>0.648271</td>\n",
       "      <td>0.396166</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>0.108045</td>\n",
       "      <td>0.144060</td>\n",
       "      <td>0.360151</td>\n",
       "      <td>0.396166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.108097</td>\n",
       "      <td>0.154424</td>\n",
       "      <td>0.205899</td>\n",
       "      <td>0.102949</td>\n",
       "      <td>0.205899</td>\n",
       "      <td>0.205899</td>\n",
       "      <td>0.308848</td>\n",
       "      <td>0.463272</td>\n",
       "      <td>0.051475</td>\n",
       "      <td>0.308848</td>\n",
       "      <td>0.154424</td>\n",
       "      <td>0.360322</td>\n",
       "      <td>0.514746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20818</th>\n",
       "      <td>2</td>\n",
       "      <td>0.162536</td>\n",
       "      <td>0.123527</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>0.097521</td>\n",
       "      <td>0.065014</td>\n",
       "      <td>0.162536</td>\n",
       "      <td>0.260057</td>\n",
       "      <td>0.487607</td>\n",
       "      <td>0.065014</td>\n",
       "      <td>0.130029</td>\n",
       "      <td>0.130029</td>\n",
       "      <td>0.325071</td>\n",
       "      <td>0.682650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20819</th>\n",
       "      <td>2</td>\n",
       "      <td>0.061498</td>\n",
       "      <td>0.110696</td>\n",
       "      <td>0.113771</td>\n",
       "      <td>0.092247</td>\n",
       "      <td>0.092247</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>0.307489</td>\n",
       "      <td>0.584228</td>\n",
       "      <td>0.030749</td>\n",
       "      <td>0.122995</td>\n",
       "      <td>0.153744</td>\n",
       "      <td>0.215242</td>\n",
       "      <td>0.645726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20820</th>\n",
       "      <td>2</td>\n",
       "      <td>0.073697</td>\n",
       "      <td>0.153535</td>\n",
       "      <td>0.184242</td>\n",
       "      <td>0.122828</td>\n",
       "      <td>0.163770</td>\n",
       "      <td>0.122828</td>\n",
       "      <td>0.409426</td>\n",
       "      <td>0.368484</td>\n",
       "      <td>0.204713</td>\n",
       "      <td>0.081885</td>\n",
       "      <td>0.122828</td>\n",
       "      <td>0.614139</td>\n",
       "      <td>0.368484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20821</th>\n",
       "      <td>2</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>0.175065</td>\n",
       "      <td>0.300111</td>\n",
       "      <td>0.066691</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.166729</td>\n",
       "      <td>0.400149</td>\n",
       "      <td>0.466840</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.100037</td>\n",
       "      <td>0.133383</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.533532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20822</th>\n",
       "      <td>2</td>\n",
       "      <td>0.055791</td>\n",
       "      <td>0.140313</td>\n",
       "      <td>0.175391</td>\n",
       "      <td>0.200447</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.100223</td>\n",
       "      <td>0.334078</td>\n",
       "      <td>0.267263</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.167039</td>\n",
       "      <td>0.467709</td>\n",
       "      <td>0.668156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20823 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       result  odds-home  odds-draw  odds-away  home-wins  home-draws  \\\n",
       "0           2   0.049957   0.165301   0.330601   0.183667    0.110200   \n",
       "1           1   0.077897   0.103862   0.121172   0.103862    0.138483   \n",
       "2           1   0.109311   0.117119   0.105407   0.195198    0.078079   \n",
       "3           0   0.068789   0.117049   0.155945   0.180075    0.108045   \n",
       "4           2   0.108097   0.154424   0.205899   0.102949    0.205899   \n",
       "...       ...        ...        ...        ...        ...         ...   \n",
       "20818       2   0.162536   0.123527   0.055262   0.097521    0.065014   \n",
       "20819       2   0.061498   0.110696   0.113771   0.092247    0.092247   \n",
       "20820       2   0.073697   0.153535   0.184242   0.122828    0.163770   \n",
       "20821       2   0.044350   0.175065   0.300111   0.066691    0.100037   \n",
       "20822       2   0.055791   0.140313   0.175391   0.200447    0.033408   \n",
       "\n",
       "       home-losses  home-goals  home-opposition-goals  away-wins  away-draws  \\\n",
       "0         0.073467    0.514268               0.367334   0.073467    0.073467   \n",
       "1         0.103862    0.553931               0.415448   0.138483    0.103862   \n",
       "2         0.117119    0.390396               0.312317   0.156158    0.156158   \n",
       "3         0.072030    0.648271               0.396166   0.108045    0.108045   \n",
       "4         0.205899    0.308848               0.463272   0.051475    0.308848   \n",
       "...            ...         ...                    ...        ...         ...   \n",
       "20818     0.162536    0.260057               0.487607   0.065014    0.130029   \n",
       "20819     0.122995    0.307489               0.584228   0.030749    0.122995   \n",
       "20820     0.122828    0.409426               0.368484   0.204713    0.081885   \n",
       "20821     0.166729    0.400149               0.466840   0.100037    0.100037   \n",
       "20822     0.100223    0.334078               0.267263   0.133631    0.033408   \n",
       "\n",
       "       away-losses  away-goals  away-opposition-goals  \n",
       "0         0.220401    0.293868               0.514268  \n",
       "1         0.103862    0.484690               0.415448  \n",
       "2         0.078079    0.585594               0.507514  \n",
       "3         0.144060    0.360151               0.396166  \n",
       "4         0.154424    0.360322               0.514746  \n",
       "...            ...         ...                    ...  \n",
       "20818     0.130029    0.325071               0.682650  \n",
       "20819     0.153744    0.215242               0.645726  \n",
       "20820     0.122828    0.614139               0.368484  \n",
       "20821     0.133383    0.366803               0.533532  \n",
       "20822     0.167039    0.467709               0.668156  \n",
       "\n",
       "[20823 rows x 14 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "UCnuh-duaSip",
    "outputId": "9ec7f29c-b38b-46c6-a6bb-d1904d8dfb17"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>odds-home</th>\n",
       "      <th>odds-draw</th>\n",
       "      <th>odds-away</th>\n",
       "      <th>home-wins</th>\n",
       "      <th>home-draws</th>\n",
       "      <th>home-losses</th>\n",
       "      <th>home-goals</th>\n",
       "      <th>home-opposition-goals</th>\n",
       "      <th>home-shots</th>\n",
       "      <th>home-shots_on_target</th>\n",
       "      <th>home-opposition_shots</th>\n",
       "      <th>home-opposition_shots_on_target</th>\n",
       "      <th>away-wins</th>\n",
       "      <th>away-draws</th>\n",
       "      <th>away-losses</th>\n",
       "      <th>away-goals</th>\n",
       "      <th>away-opposition-goals</th>\n",
       "      <th>away-shots</th>\n",
       "      <th>away-shots_on_target</th>\n",
       "      <th>away-opposition_shots</th>\n",
       "      <th>away-opposition_shots_on_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.011769</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.039230</td>\n",
       "      <td>0.057061</td>\n",
       "      <td>0.488587</td>\n",
       "      <td>0.238944</td>\n",
       "      <td>0.417260</td>\n",
       "      <td>0.189015</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053495</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.574178</td>\n",
       "      <td>0.278173</td>\n",
       "      <td>0.256775</td>\n",
       "      <td>0.106990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009236</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.022166</td>\n",
       "      <td>0.029555</td>\n",
       "      <td>0.059110</td>\n",
       "      <td>0.495044</td>\n",
       "      <td>0.236439</td>\n",
       "      <td>0.557848</td>\n",
       "      <td>0.284465</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>0.040638</td>\n",
       "      <td>0.066498</td>\n",
       "      <td>0.384213</td>\n",
       "      <td>0.162552</td>\n",
       "      <td>0.321409</td>\n",
       "      <td>0.132997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>0.037632</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.451585</td>\n",
       "      <td>0.218266</td>\n",
       "      <td>0.466638</td>\n",
       "      <td>0.210740</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.022579</td>\n",
       "      <td>0.041395</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.504270</td>\n",
       "      <td>0.222029</td>\n",
       "      <td>0.376321</td>\n",
       "      <td>0.173108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.007281</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.006332</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.069647</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>0.560339</td>\n",
       "      <td>0.259592</td>\n",
       "      <td>0.234266</td>\n",
       "      <td>0.117133</td>\n",
       "      <td>0.018995</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.060149</td>\n",
       "      <td>0.025326</td>\n",
       "      <td>0.535013</td>\n",
       "      <td>0.300747</td>\n",
       "      <td>0.357730</td>\n",
       "      <td>0.183614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.064551</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.050961</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>0.546982</td>\n",
       "      <td>0.244613</td>\n",
       "      <td>0.251408</td>\n",
       "      <td>0.105320</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.033974</td>\n",
       "      <td>0.057756</td>\n",
       "      <td>0.485829</td>\n",
       "      <td>0.234421</td>\n",
       "      <td>0.455252</td>\n",
       "      <td>0.234421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7028</th>\n",
       "      <td>2</td>\n",
       "      <td>0.020289</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.048694</td>\n",
       "      <td>0.324626</td>\n",
       "      <td>0.174487</td>\n",
       "      <td>0.474766</td>\n",
       "      <td>0.243470</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.085214</td>\n",
       "      <td>0.482881</td>\n",
       "      <td>0.235354</td>\n",
       "      <td>0.454477</td>\n",
       "      <td>0.263759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>2</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.017929</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.014941</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.024901</td>\n",
       "      <td>0.044822</td>\n",
       "      <td>0.104584</td>\n",
       "      <td>0.443238</td>\n",
       "      <td>0.234070</td>\n",
       "      <td>0.458179</td>\n",
       "      <td>0.229089</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.019921</td>\n",
       "      <td>0.024901</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>0.104584</td>\n",
       "      <td>0.517941</td>\n",
       "      <td>0.273911</td>\n",
       "      <td>0.313753</td>\n",
       "      <td>0.129485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>2</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>0.033855</td>\n",
       "      <td>0.033855</td>\n",
       "      <td>0.516293</td>\n",
       "      <td>0.249683</td>\n",
       "      <td>0.389336</td>\n",
       "      <td>0.211596</td>\n",
       "      <td>0.021160</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.063479</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.355481</td>\n",
       "      <td>0.181972</td>\n",
       "      <td>0.499366</td>\n",
       "      <td>0.236987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.055483</td>\n",
       "      <td>0.491416</td>\n",
       "      <td>0.245708</td>\n",
       "      <td>0.392341</td>\n",
       "      <td>0.198152</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>0.043593</td>\n",
       "      <td>0.067372</td>\n",
       "      <td>0.408193</td>\n",
       "      <td>0.210041</td>\n",
       "      <td>0.483490</td>\n",
       "      <td>0.214004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>0.023501</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.039168</td>\n",
       "      <td>0.031334</td>\n",
       "      <td>0.411259</td>\n",
       "      <td>0.254589</td>\n",
       "      <td>0.493511</td>\n",
       "      <td>0.211505</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>0.019584</td>\n",
       "      <td>0.050918</td>\n",
       "      <td>0.074418</td>\n",
       "      <td>0.446510</td>\n",
       "      <td>0.246755</td>\n",
       "      <td>0.415176</td>\n",
       "      <td>0.180171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7033 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      result  odds-home  odds-draw  odds-away  home-wins  home-draws  \\\n",
       "0          2   0.012482   0.011769   0.007489   0.003566    0.010699   \n",
       "1          1   0.009236   0.012191   0.010640   0.011083    0.003694   \n",
       "2          0   0.007188   0.012795   0.015805   0.015053    0.007526   \n",
       "3          2   0.010289   0.010289   0.007281   0.015829    0.006332   \n",
       "4          2   0.004077   0.020384   0.064551   0.023782    0.006795   \n",
       "...      ...        ...        ...        ...        ...         ...   \n",
       "7028       2   0.020289   0.015420   0.006898   0.016231    0.008116   \n",
       "7029       2   0.009960   0.017929   0.018427   0.014941    0.009960   \n",
       "7030       2   0.007617   0.015870   0.019044   0.012696    0.012696   \n",
       "7031       2   0.005271   0.020806   0.035667   0.007926    0.011889   \n",
       "7032       2   0.006541   0.016450   0.020563   0.023501    0.003917   \n",
       "\n",
       "      home-losses  home-goals  home-opposition-goals  home-shots  \\\n",
       "0        0.021398    0.039230               0.057061    0.488587   \n",
       "1        0.022166    0.029555               0.059110    0.495044   \n",
       "2        0.015053    0.037632               0.056448    0.451585   \n",
       "3        0.009497    0.069647               0.037989    0.560339   \n",
       "4        0.003397    0.050961               0.027179    0.546982   \n",
       "...           ...         ...                    ...         ...   \n",
       "7028     0.016231    0.040578               0.048694    0.324626   \n",
       "7029     0.024901    0.044822               0.104584    0.443238   \n",
       "7030     0.016928    0.033855               0.033855    0.516293   \n",
       "7031     0.019815    0.047556               0.055483    0.491416   \n",
       "7032     0.011750    0.039168               0.031334    0.411259   \n",
       "\n",
       "      home-shots_on_target  home-opposition_shots  \\\n",
       "0                 0.238944               0.417260   \n",
       "1                 0.236439               0.557848   \n",
       "2                 0.218266               0.466638   \n",
       "3                 0.259592               0.234266   \n",
       "4                 0.244613               0.251408   \n",
       "...                    ...                    ...   \n",
       "7028              0.174487               0.474766   \n",
       "7029              0.234070               0.458179   \n",
       "7030              0.249683               0.389336   \n",
       "7031              0.245708               0.392341   \n",
       "7032              0.254589               0.493511   \n",
       "\n",
       "      home-opposition_shots_on_target  away-wins  away-draws  away-losses  \\\n",
       "0                            0.189015   0.028531    0.007133     0.000000   \n",
       "1                            0.284465   0.011083    0.011083     0.014777   \n",
       "2                            0.210740   0.007526    0.007526     0.022579   \n",
       "3                            0.117133   0.018995    0.009497     0.003166   \n",
       "4                            0.105320   0.010192    0.006795     0.016987   \n",
       "...                               ...        ...         ...          ...   \n",
       "7028                         0.243470   0.008116    0.016231     0.016231   \n",
       "7029                         0.229089   0.004980    0.019921     0.024901   \n",
       "7030                         0.211596   0.021160    0.008464     0.012696   \n",
       "7031                         0.198152   0.011889    0.011889     0.015852   \n",
       "7032                         0.211505   0.011750    0.007834     0.019584   \n",
       "\n",
       "      away-goals  away-opposition-goals  away-shots  away-shots_on_target  \\\n",
       "0       0.053495               0.021398    0.574178              0.278173   \n",
       "1       0.040638               0.066498    0.384213              0.162552   \n",
       "2       0.041395               0.056448    0.504270              0.222029   \n",
       "3       0.060149               0.025326    0.535013              0.300747   \n",
       "4       0.033974               0.057756    0.485829              0.234421   \n",
       "...          ...                    ...         ...                   ...   \n",
       "7028    0.040578               0.085214    0.482881              0.235354   \n",
       "7029    0.034861               0.104584    0.517941              0.273911   \n",
       "7030    0.063479               0.038087    0.355481              0.181972   \n",
       "7031    0.043593               0.067372    0.408193              0.210041   \n",
       "7032    0.050918               0.074418    0.446510              0.246755   \n",
       "\n",
       "      away-opposition_shots  away-opposition_shots_on_target  \n",
       "0                  0.256775                         0.106990  \n",
       "1                  0.321409                         0.132997  \n",
       "2                  0.376321                         0.173108  \n",
       "3                  0.357730                         0.183614  \n",
       "4                  0.455252                         0.234421  \n",
       "...                     ...                              ...  \n",
       "7028               0.454477                         0.263759  \n",
       "7029               0.313753                         0.129485  \n",
       "7030               0.499366                         0.236987  \n",
       "7031               0.483490                         0.214004  \n",
       "7032               0.415176                         0.180171  \n",
       "\n",
       "[7033 rows x 22 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "iUgEWCcJaTKx",
    "outputId": "a30e80e8-8b13-43cf-988d-0fa22e73fd94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>odds-home</th>\n",
       "      <th>odds-draw</th>\n",
       "      <th>odds-away</th>\n",
       "      <th>home-wins</th>\n",
       "      <th>home-draws</th>\n",
       "      <th>home-losses</th>\n",
       "      <th>home-goals</th>\n",
       "      <th>home-opposition-goals</th>\n",
       "      <th>home-shots</th>\n",
       "      <th>home-shots_on_target</th>\n",
       "      <th>home-opposition_shots</th>\n",
       "      <th>home-opposition_shots_on_target</th>\n",
       "      <th>away-wins</th>\n",
       "      <th>away-draws</th>\n",
       "      <th>away-losses</th>\n",
       "      <th>away-goals</th>\n",
       "      <th>away-opposition-goals</th>\n",
       "      <th>away-shots</th>\n",
       "      <th>away-shots_on_target</th>\n",
       "      <th>away-opposition_shots</th>\n",
       "      <th>away-opposition_shots_on_target</th>\n",
       "      <th>home_shot_accuracy</th>\n",
       "      <th>home_shot_efficiency</th>\n",
       "      <th>home_opposition_shot_accuracy</th>\n",
       "      <th>home_opposition_shot_efficiency</th>\n",
       "      <th>away_shot_accuracy</th>\n",
       "      <th>away_shot_efficiency</th>\n",
       "      <th>away_opposition_shot_accuracy</th>\n",
       "      <th>away_opposition_shot_efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.011769</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.039229</td>\n",
       "      <td>0.057061</td>\n",
       "      <td>0.488583</td>\n",
       "      <td>0.238942</td>\n",
       "      <td>0.417257</td>\n",
       "      <td>0.189014</td>\n",
       "      <td>0.028530</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053495</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.574174</td>\n",
       "      <td>0.278171</td>\n",
       "      <td>0.256774</td>\n",
       "      <td>0.106989</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.000713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.009236</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.010640</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.022166</td>\n",
       "      <td>0.029555</td>\n",
       "      <td>0.059109</td>\n",
       "      <td>0.495039</td>\n",
       "      <td>0.236437</td>\n",
       "      <td>0.557843</td>\n",
       "      <td>0.284463</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.014777</td>\n",
       "      <td>0.040638</td>\n",
       "      <td>0.066498</td>\n",
       "      <td>0.384210</td>\n",
       "      <td>0.162550</td>\n",
       "      <td>0.321406</td>\n",
       "      <td>0.132996</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.012795</td>\n",
       "      <td>0.015805</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>0.037632</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.451581</td>\n",
       "      <td>0.218264</td>\n",
       "      <td>0.466634</td>\n",
       "      <td>0.210738</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>0.022579</td>\n",
       "      <td>0.041395</td>\n",
       "      <td>0.056448</td>\n",
       "      <td>0.504266</td>\n",
       "      <td>0.222028</td>\n",
       "      <td>0.376318</td>\n",
       "      <td>0.173106</td>\n",
       "      <td>0.001819</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.001227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>0.007281</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.006331</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.069646</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>0.560335</td>\n",
       "      <td>0.259590</td>\n",
       "      <td>0.234264</td>\n",
       "      <td>0.117132</td>\n",
       "      <td>0.018994</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.060149</td>\n",
       "      <td>0.025326</td>\n",
       "      <td>0.535009</td>\n",
       "      <td>0.300745</td>\n",
       "      <td>0.357728</td>\n",
       "      <td>0.183613</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>0.064550</td>\n",
       "      <td>0.023782</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.050961</td>\n",
       "      <td>0.027179</td>\n",
       "      <td>0.546979</td>\n",
       "      <td>0.244612</td>\n",
       "      <td>0.251406</td>\n",
       "      <td>0.105319</td>\n",
       "      <td>0.010192</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.016987</td>\n",
       "      <td>0.033974</td>\n",
       "      <td>0.057756</td>\n",
       "      <td>0.485826</td>\n",
       "      <td>0.234419</td>\n",
       "      <td>0.455249</td>\n",
       "      <td>0.234419</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.000837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7028</th>\n",
       "      <td>2</td>\n",
       "      <td>0.020289</td>\n",
       "      <td>0.015420</td>\n",
       "      <td>0.006898</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>0.324623</td>\n",
       "      <td>0.174485</td>\n",
       "      <td>0.474761</td>\n",
       "      <td>0.243467</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.085213</td>\n",
       "      <td>0.482876</td>\n",
       "      <td>0.235351</td>\n",
       "      <td>0.454472</td>\n",
       "      <td>0.263756</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7029</th>\n",
       "      <td>2</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>0.018426</td>\n",
       "      <td>0.014940</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.044821</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>0.443228</td>\n",
       "      <td>0.234064</td>\n",
       "      <td>0.458168</td>\n",
       "      <td>0.229084</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>0.517929</td>\n",
       "      <td>0.273905</td>\n",
       "      <td>0.313745</td>\n",
       "      <td>0.129482</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.004022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>2</td>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.016927</td>\n",
       "      <td>0.033855</td>\n",
       "      <td>0.033855</td>\n",
       "      <td>0.516288</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>0.389332</td>\n",
       "      <td>0.211593</td>\n",
       "      <td>0.021159</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.012696</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.355477</td>\n",
       "      <td>0.181970</td>\n",
       "      <td>0.499360</td>\n",
       "      <td>0.236985</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.002166</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7031</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.035667</td>\n",
       "      <td>0.007926</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.047556</td>\n",
       "      <td>0.055482</td>\n",
       "      <td>0.491412</td>\n",
       "      <td>0.245706</td>\n",
       "      <td>0.392337</td>\n",
       "      <td>0.198150</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.011889</td>\n",
       "      <td>0.015852</td>\n",
       "      <td>0.043593</td>\n",
       "      <td>0.067371</td>\n",
       "      <td>0.408189</td>\n",
       "      <td>0.210039</td>\n",
       "      <td>0.483486</td>\n",
       "      <td>0.214002</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.001248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7032</th>\n",
       "      <td>2</td>\n",
       "      <td>0.006541</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.020563</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.039167</td>\n",
       "      <td>0.031334</td>\n",
       "      <td>0.411255</td>\n",
       "      <td>0.254586</td>\n",
       "      <td>0.493506</td>\n",
       "      <td>0.211503</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.007833</td>\n",
       "      <td>0.019584</td>\n",
       "      <td>0.050917</td>\n",
       "      <td>0.074418</td>\n",
       "      <td>0.446505</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.415172</td>\n",
       "      <td>0.180169</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7033 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      result  odds-home  odds-draw  odds-away  home-wins  home-draws  \\\n",
       "0          2   0.012482   0.011769   0.007489   0.003566    0.010699   \n",
       "1          1   0.009236   0.012191   0.010640   0.011083    0.003694   \n",
       "2          0   0.007188   0.012795   0.015805   0.015053    0.007526   \n",
       "3          2   0.010289   0.010289   0.007281   0.015829    0.006331   \n",
       "4          2   0.004077   0.020384   0.064550   0.023782    0.006795   \n",
       "...      ...        ...        ...        ...        ...         ...   \n",
       "7028       2   0.020289   0.015420   0.006898   0.016231    0.008116   \n",
       "7029       2   0.009960   0.017928   0.018426   0.014940    0.009960   \n",
       "7030       2   0.007617   0.015870   0.019043   0.012696    0.012696   \n",
       "7031       2   0.005271   0.020806   0.035667   0.007926    0.011889   \n",
       "7032       2   0.006541   0.016450   0.020563   0.023500    0.003917   \n",
       "\n",
       "      home-losses  home-goals  home-opposition-goals  home-shots  \\\n",
       "0        0.021398    0.039229               0.057061    0.488583   \n",
       "1        0.022166    0.029555               0.059109    0.495039   \n",
       "2        0.015053    0.037632               0.056448    0.451581   \n",
       "3        0.009497    0.069646               0.037989    0.560335   \n",
       "4        0.003397    0.050961               0.027179    0.546979   \n",
       "...           ...         ...                    ...         ...   \n",
       "7028     0.016231    0.040578               0.048693    0.324623   \n",
       "7029     0.024900    0.044821               0.104582    0.443228   \n",
       "7030     0.016927    0.033855               0.033855    0.516288   \n",
       "7031     0.019815    0.047556               0.055482    0.491412   \n",
       "7032     0.011750    0.039167               0.031334    0.411255   \n",
       "\n",
       "      home-shots_on_target  home-opposition_shots  \\\n",
       "0                 0.238942               0.417257   \n",
       "1                 0.236437               0.557843   \n",
       "2                 0.218264               0.466634   \n",
       "3                 0.259590               0.234264   \n",
       "4                 0.244612               0.251406   \n",
       "...                    ...                    ...   \n",
       "7028              0.174485               0.474761   \n",
       "7029              0.234064               0.458168   \n",
       "7030              0.249680               0.389332   \n",
       "7031              0.245706               0.392337   \n",
       "7032              0.254586               0.493506   \n",
       "\n",
       "      home-opposition_shots_on_target  away-wins  away-draws  away-losses  \\\n",
       "0                            0.189014   0.028530    0.007133     0.000000   \n",
       "1                            0.284463   0.011083    0.011083     0.014777   \n",
       "2                            0.210738   0.007526    0.007526     0.022579   \n",
       "3                            0.117132   0.018994    0.009497     0.003166   \n",
       "4                            0.105319   0.010192    0.006795     0.016987   \n",
       "...                               ...        ...         ...          ...   \n",
       "7028                         0.243467   0.008116    0.016231     0.016231   \n",
       "7029                         0.229084   0.004980    0.019920     0.024900   \n",
       "7030                         0.211593   0.021159    0.008464     0.012696   \n",
       "7031                         0.198150   0.011889    0.011889     0.015852   \n",
       "7032                         0.211503   0.011750    0.007833     0.019584   \n",
       "\n",
       "      away-goals  away-opposition-goals  away-shots  away-shots_on_target  \\\n",
       "0       0.053495               0.021398    0.574174              0.278171   \n",
       "1       0.040638               0.066498    0.384210              0.162550   \n",
       "2       0.041395               0.056448    0.504266              0.222028   \n",
       "3       0.060149               0.025326    0.535009              0.300745   \n",
       "4       0.033974               0.057756    0.485826              0.234419   \n",
       "...          ...                    ...         ...                   ...   \n",
       "7028    0.040578               0.085213    0.482876              0.235351   \n",
       "7029    0.034861               0.104582    0.517929              0.273905   \n",
       "7030    0.063478               0.038087    0.355477              0.181970   \n",
       "7031    0.043593               0.067371    0.408189              0.210039   \n",
       "7032    0.050917               0.074418    0.446505              0.246753   \n",
       "\n",
       "      away-opposition_shots  away-opposition_shots_on_target  \\\n",
       "0                  0.256774                         0.106989   \n",
       "1                  0.321406                         0.132996   \n",
       "2                  0.376318                         0.173106   \n",
       "3                  0.357728                         0.183613   \n",
       "4                  0.455249                         0.234419   \n",
       "...                     ...                              ...   \n",
       "7028               0.454472                         0.263756   \n",
       "7029               0.313745                         0.129482   \n",
       "7030               0.499360                         0.236985   \n",
       "7031               0.483486                         0.214002   \n",
       "7032               0.415172                         0.180169   \n",
       "\n",
       "      home_shot_accuracy  home_shot_efficiency  home_opposition_shot_accuracy  \\\n",
       "0               0.001744              0.000586                       0.001616   \n",
       "1               0.001764              0.000462                       0.001884   \n",
       "2               0.001819              0.000649                       0.001699   \n",
       "3               0.001467              0.000849                       0.001583   \n",
       "4               0.001519              0.000708                       0.001423   \n",
       "...                  ...                   ...                            ...   \n",
       "7028            0.002181              0.000944                       0.002081   \n",
       "7029            0.002630              0.000954                       0.002490   \n",
       "7030            0.002047              0.000574                       0.002300   \n",
       "7031            0.001981              0.000767                       0.002002   \n",
       "7032            0.002425              0.000603                       0.001679   \n",
       "\n",
       "      home_opposition_shot_efficiency  away_shot_accuracy  \\\n",
       "0                            0.001077            0.001728   \n",
       "1                            0.000768            0.001563   \n",
       "2                            0.001008            0.001657   \n",
       "3                            0.001027            0.001780   \n",
       "4                            0.000877            0.001639   \n",
       "...                               ...                 ...   \n",
       "7028                         0.000812            0.001978   \n",
       "7029                         0.002274            0.002634   \n",
       "7030                         0.000677            0.002166   \n",
       "7031                         0.001110            0.002039   \n",
       "7032                         0.000580            0.002164   \n",
       "\n",
       "      away_shot_efficiency  away_opposition_shot_accuracy  \\\n",
       "0                 0.000686                       0.001486   \n",
       "1                 0.000924                       0.001529   \n",
       "2                 0.000702                       0.001731   \n",
       "3                 0.000633                       0.001625   \n",
       "4                 0.000492                       0.001749   \n",
       "...                    ...                            ...   \n",
       "7028              0.000700                       0.002355   \n",
       "7029              0.000634                       0.002055   \n",
       "7030              0.001476                       0.002008   \n",
       "7031              0.000823                       0.001754   \n",
       "7032              0.000808                       0.001700   \n",
       "\n",
       "      away_opposition_shot_efficiency  \n",
       "0                            0.000713  \n",
       "1                            0.001847  \n",
       "2                            0.001227  \n",
       "3                            0.000437  \n",
       "4                            0.000837  \n",
       "...                               ...  \n",
       "7028                         0.001311  \n",
       "7029                         0.004022  \n",
       "7030                         0.000680  \n",
       "7031                         0.001248  \n",
       "7032                         0.001618  \n",
       "\n",
       "[7033 rows x 30 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "tPTv1eKnaTy6",
    "outputId": "a438f9c7-fdce-4e67-a4fc-8af1bf8e89e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>odds-home</th>\n",
       "      <th>odds-draw</th>\n",
       "      <th>odds-away</th>\n",
       "      <th>home-wins</th>\n",
       "      <th>home-draws</th>\n",
       "      <th>home-losses</th>\n",
       "      <th>home-goals</th>\n",
       "      <th>home-opposition-goals</th>\n",
       "      <th>home-shots</th>\n",
       "      <th>home-shots_on_target</th>\n",
       "      <th>home-possession</th>\n",
       "      <th>home-opposition_shots</th>\n",
       "      <th>home-opposition_shots_on_target</th>\n",
       "      <th>home-opposition_possession</th>\n",
       "      <th>away-wins</th>\n",
       "      <th>away-draws</th>\n",
       "      <th>away-losses</th>\n",
       "      <th>away-goals</th>\n",
       "      <th>away-opposition-goals</th>\n",
       "      <th>away-shots</th>\n",
       "      <th>away-shots_on_target</th>\n",
       "      <th>away-possession</th>\n",
       "      <th>away-opposition_shots</th>\n",
       "      <th>away-opposition_shots_on_target</th>\n",
       "      <th>away-opposition_possession</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.143652</td>\n",
       "      <td>0.070253</td>\n",
       "      <td>0.504354</td>\n",
       "      <td>0.122681</td>\n",
       "      <td>0.055573</td>\n",
       "      <td>0.449829</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.168817</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.505403</td>\n",
       "      <td>0.075496</td>\n",
       "      <td>0.031457</td>\n",
       "      <td>0.448781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.192540</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.550114</td>\n",
       "      <td>0.070292</td>\n",
       "      <td>0.036674</td>\n",
       "      <td>0.375911</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.172165</td>\n",
       "      <td>0.096779</td>\n",
       "      <td>0.558264</td>\n",
       "      <td>0.115116</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>0.368780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.015678</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.168280</td>\n",
       "      <td>0.075256</td>\n",
       "      <td>0.498568</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>0.032402</td>\n",
       "      <td>0.452578</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.137968</td>\n",
       "      <td>0.064803</td>\n",
       "      <td>0.443171</td>\n",
       "      <td>0.145285</td>\n",
       "      <td>0.074210</td>\n",
       "      <td>0.507975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.107092</td>\n",
       "      <td>0.049346</td>\n",
       "      <td>0.460917</td>\n",
       "      <td>0.139640</td>\n",
       "      <td>0.080844</td>\n",
       "      <td>0.494515</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.430469</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.067195</td>\n",
       "      <td>0.524963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.130540</td>\n",
       "      <td>0.065792</td>\n",
       "      <td>0.454281</td>\n",
       "      <td>0.159781</td>\n",
       "      <td>0.076236</td>\n",
       "      <td>0.496054</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.013576</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.107565</td>\n",
       "      <td>0.053260</td>\n",
       "      <td>0.456369</td>\n",
       "      <td>0.144117</td>\n",
       "      <td>0.079369</td>\n",
       "      <td>0.493965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.084331</td>\n",
       "      <td>0.045328</td>\n",
       "      <td>0.436415</td>\n",
       "      <td>0.123335</td>\n",
       "      <td>0.063249</td>\n",
       "      <td>0.521801</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.125443</td>\n",
       "      <td>0.061140</td>\n",
       "      <td>0.410062</td>\n",
       "      <td>0.118064</td>\n",
       "      <td>0.068519</td>\n",
       "      <td>0.549208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.050129</td>\n",
       "      <td>0.417030</td>\n",
       "      <td>0.098125</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.553551</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.110924</td>\n",
       "      <td>0.058662</td>\n",
       "      <td>0.447961</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>0.027731</td>\n",
       "      <td>0.522621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.129584</td>\n",
       "      <td>0.062667</td>\n",
       "      <td>0.476910</td>\n",
       "      <td>0.097719</td>\n",
       "      <td>0.053108</td>\n",
       "      <td>0.489656</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>0.089221</td>\n",
       "      <td>0.045673</td>\n",
       "      <td>0.525769</td>\n",
       "      <td>0.125335</td>\n",
       "      <td>0.059481</td>\n",
       "      <td>0.439734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>0.009468</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.130447</td>\n",
       "      <td>0.065223</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.104147</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.399757</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>0.017884</td>\n",
       "      <td>0.108355</td>\n",
       "      <td>0.055756</td>\n",
       "      <td>0.460772</td>\n",
       "      <td>0.128343</td>\n",
       "      <td>0.056808</td>\n",
       "      <td>0.496540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.110153</td>\n",
       "      <td>0.068190</td>\n",
       "      <td>0.568601</td>\n",
       "      <td>0.132184</td>\n",
       "      <td>0.056650</td>\n",
       "      <td>0.386061</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>0.019932</td>\n",
       "      <td>0.119595</td>\n",
       "      <td>0.066092</td>\n",
       "      <td>0.487822</td>\n",
       "      <td>0.111202</td>\n",
       "      <td>0.048258</td>\n",
       "      <td>0.466840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6996 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      result  odds-home  odds-draw  odds-away  home-wins  home-draws  \\\n",
       "0          2   0.003670   0.003460   0.002202   0.001049    0.003146   \n",
       "1          2   0.003311   0.003311   0.002343   0.005094    0.002037   \n",
       "2          2   0.001254   0.006271   0.019859   0.007317    0.002090   \n",
       "3          0   0.002499   0.003465   0.003150   0.003150    0.003150   \n",
       "4          0   0.002715   0.003394   0.002924   0.004177    0.001044   \n",
       "...      ...        ...        ...        ...        ...         ...   \n",
       "6991       2   0.005271   0.004006   0.001792   0.004217    0.002108   \n",
       "6992       2   0.002133   0.003840   0.003946   0.003200    0.002133   \n",
       "6993       2   0.001912   0.003983   0.004780   0.003186    0.003186   \n",
       "6994       2   0.001399   0.005523   0.009468   0.002104    0.003156   \n",
       "6995       2   0.001752   0.004406   0.005508   0.006294    0.001049   \n",
       "\n",
       "      home-losses  home-goals  home-opposition-goals  home-shots  \\\n",
       "0        0.006291    0.011534               0.016777    0.143652   \n",
       "1        0.003056    0.019356               0.012225    0.192540   \n",
       "2        0.001045    0.015678               0.008362    0.168280   \n",
       "3        0.004200    0.009449               0.011549    0.107092   \n",
       "4        0.005222    0.015665               0.016709    0.130540   \n",
       "...           ...         ...                    ...         ...   \n",
       "6991     0.004217    0.010541               0.012650    0.084331   \n",
       "6992     0.005333    0.009599               0.022398    0.094925   \n",
       "6993     0.004249    0.008497               0.008497    0.129584   \n",
       "6994     0.005260    0.012624               0.014728    0.130447   \n",
       "6995     0.003147    0.010491               0.008393    0.110153   \n",
       "\n",
       "      home-shots_on_target  home-possession  home-opposition_shots  \\\n",
       "0                 0.070253         0.504354               0.122681   \n",
       "1                 0.090667         0.550114               0.070292   \n",
       "2                 0.075256         0.498568               0.077346   \n",
       "3                 0.049346         0.460917               0.139640   \n",
       "4                 0.065792         0.454281               0.159781   \n",
       "...                    ...              ...                    ...   \n",
       "6991              0.045328         0.436415               0.123335   \n",
       "6992              0.050129         0.417030               0.098125   \n",
       "6993              0.062667         0.476910               0.097719   \n",
       "6994              0.065223         0.557555               0.104147   \n",
       "6995              0.068190         0.568601               0.132184   \n",
       "\n",
       "      home-opposition_shots_on_target  home-opposition_possession  away-wins  \\\n",
       "0                            0.055573                    0.449829   0.008388   \n",
       "1                            0.036674                    0.375911   0.006112   \n",
       "2                            0.032402                    0.452578   0.002090   \n",
       "3                            0.080844                    0.494515   0.003150   \n",
       "4                            0.076236                    0.496054   0.004177   \n",
       "...                               ...                         ...        ...   \n",
       "6991                         0.063249                    0.521801   0.002108   \n",
       "6992                         0.049062                    0.553551   0.001067   \n",
       "6993                         0.053108                    0.489656   0.005311   \n",
       "6994                         0.052600                    0.399757   0.003156   \n",
       "6995                         0.056650                    0.386061   0.003147   \n",
       "\n",
       "      away-draws  away-losses  away-goals  away-opposition-goals  away-shots  \\\n",
       "0       0.002097     0.000000    0.015728               0.006291    0.168817   \n",
       "1       0.003056     0.001019    0.019356               0.008150    0.172165   \n",
       "2       0.002090     0.006271    0.009407               0.018814    0.137968   \n",
       "3       0.002100     0.005250    0.009449               0.016799    0.121791   \n",
       "4       0.003133     0.003133    0.013576               0.016709    0.107565   \n",
       "...          ...          ...         ...                    ...         ...   \n",
       "6991    0.004217     0.004217    0.010541               0.022137    0.125443   \n",
       "6992    0.004266     0.005333    0.007466               0.022398    0.110924   \n",
       "6993    0.002124     0.003186    0.015932               0.009559    0.089221   \n",
       "6994    0.003156     0.004208    0.011572               0.017884    0.108355   \n",
       "6995    0.002098     0.005245    0.013638               0.019932    0.119595   \n",
       "\n",
       "      away-shots_on_target  away-possession  away-opposition_shots  \\\n",
       "0                 0.081787         0.505403               0.075496   \n",
       "1                 0.096779         0.558264               0.115116   \n",
       "2                 0.064803         0.443171               0.145285   \n",
       "3                 0.056696         0.430469               0.137540   \n",
       "4                 0.053260         0.456369               0.144117   \n",
       "...                    ...              ...                    ...   \n",
       "6991              0.061140         0.410062               0.118064   \n",
       "6992              0.058662         0.447961               0.067194   \n",
       "6993              0.045673         0.525769               0.125335   \n",
       "6994              0.055756         0.460772               0.128343   \n",
       "6995              0.066092         0.487822               0.111202   \n",
       "\n",
       "      away-opposition_shots_on_target  away-opposition_possession  \n",
       "0                            0.031457                    0.448781  \n",
       "1                            0.059086                    0.368780  \n",
       "2                            0.074210                    0.507975  \n",
       "3                            0.067195                    0.524963  \n",
       "4                            0.079369                    0.493965  \n",
       "...                               ...                         ...  \n",
       "6991                         0.068519                    0.549208  \n",
       "6992                         0.027731                    0.522621  \n",
       "6993                         0.059481                    0.439734  \n",
       "6994                         0.056808                    0.496540  \n",
       "6995                         0.048258                    0.466840  \n",
       "\n",
       "[6996 rows x 26 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "wHXHdefzaUcP",
    "outputId": "55ce72c7-6329-4541-c391-a10303b02d3c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>odds-home</th>\n",
       "      <th>odds-draw</th>\n",
       "      <th>odds-away</th>\n",
       "      <th>home-wins</th>\n",
       "      <th>home-draws</th>\n",
       "      <th>home-losses</th>\n",
       "      <th>home-goals</th>\n",
       "      <th>home-opposition-goals</th>\n",
       "      <th>home-shots</th>\n",
       "      <th>home-shots_on_target</th>\n",
       "      <th>home-possession</th>\n",
       "      <th>home-opposition_shots</th>\n",
       "      <th>home-opposition_shots_on_target</th>\n",
       "      <th>home-opposition_possession</th>\n",
       "      <th>away-wins</th>\n",
       "      <th>away-draws</th>\n",
       "      <th>away-losses</th>\n",
       "      <th>away-goals</th>\n",
       "      <th>away-opposition-goals</th>\n",
       "      <th>away-shots</th>\n",
       "      <th>away-shots_on_target</th>\n",
       "      <th>away-possession</th>\n",
       "      <th>away-opposition_shots</th>\n",
       "      <th>away-opposition_shots_on_target</th>\n",
       "      <th>away-opposition_possession</th>\n",
       "      <th>home_shot_accuracy</th>\n",
       "      <th>home_shot_efficiency</th>\n",
       "      <th>home_opposition_shot_accuracy</th>\n",
       "      <th>home_opposition_shot_efficiency</th>\n",
       "      <th>away_shot_accuracy</th>\n",
       "      <th>away_shot_efficiency</th>\n",
       "      <th>away_opposition_shot_accuracy</th>\n",
       "      <th>away_opposition_shot_efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003670</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.011534</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.143652</td>\n",
       "      <td>0.070253</td>\n",
       "      <td>0.504354</td>\n",
       "      <td>0.122681</td>\n",
       "      <td>0.055573</td>\n",
       "      <td>0.449829</td>\n",
       "      <td>0.008388</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.168817</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.505402</td>\n",
       "      <td>0.075496</td>\n",
       "      <td>0.031457</td>\n",
       "      <td>0.448780</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.005094</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.192540</td>\n",
       "      <td>0.090667</td>\n",
       "      <td>0.550114</td>\n",
       "      <td>0.070292</td>\n",
       "      <td>0.036674</td>\n",
       "      <td>0.375911</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.019356</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.172165</td>\n",
       "      <td>0.096779</td>\n",
       "      <td>0.558263</td>\n",
       "      <td>0.115116</td>\n",
       "      <td>0.059086</td>\n",
       "      <td>0.368780</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.015678</td>\n",
       "      <td>0.008362</td>\n",
       "      <td>0.168280</td>\n",
       "      <td>0.075255</td>\n",
       "      <td>0.498567</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>0.032402</td>\n",
       "      <td>0.452578</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.006271</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.137968</td>\n",
       "      <td>0.064803</td>\n",
       "      <td>0.443171</td>\n",
       "      <td>0.145285</td>\n",
       "      <td>0.074210</td>\n",
       "      <td>0.507974</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.107092</td>\n",
       "      <td>0.049346</td>\n",
       "      <td>0.460917</td>\n",
       "      <td>0.139640</td>\n",
       "      <td>0.080844</td>\n",
       "      <td>0.494515</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.016799</td>\n",
       "      <td>0.121791</td>\n",
       "      <td>0.056696</td>\n",
       "      <td>0.430469</td>\n",
       "      <td>0.137540</td>\n",
       "      <td>0.067195</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.015665</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.130540</td>\n",
       "      <td>0.065792</td>\n",
       "      <td>0.454280</td>\n",
       "      <td>0.159781</td>\n",
       "      <td>0.076236</td>\n",
       "      <td>0.496053</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.003133</td>\n",
       "      <td>0.013576</td>\n",
       "      <td>0.016709</td>\n",
       "      <td>0.107565</td>\n",
       "      <td>0.053260</td>\n",
       "      <td>0.456369</td>\n",
       "      <td>0.144117</td>\n",
       "      <td>0.079369</td>\n",
       "      <td>0.493965</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6991</th>\n",
       "      <td>2</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.004006</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.012650</td>\n",
       "      <td>0.084331</td>\n",
       "      <td>0.045328</td>\n",
       "      <td>0.436415</td>\n",
       "      <td>0.123335</td>\n",
       "      <td>0.063249</td>\n",
       "      <td>0.521800</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.010541</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>0.125443</td>\n",
       "      <td>0.061140</td>\n",
       "      <td>0.410061</td>\n",
       "      <td>0.118064</td>\n",
       "      <td>0.068519</td>\n",
       "      <td>0.549208</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6992</th>\n",
       "      <td>2</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.050129</td>\n",
       "      <td>0.417030</td>\n",
       "      <td>0.098125</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.553551</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.007466</td>\n",
       "      <td>0.022398</td>\n",
       "      <td>0.110923</td>\n",
       "      <td>0.058661</td>\n",
       "      <td>0.447960</td>\n",
       "      <td>0.067194</td>\n",
       "      <td>0.027731</td>\n",
       "      <td>0.522620</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.004780</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.129583</td>\n",
       "      <td>0.062667</td>\n",
       "      <td>0.476909</td>\n",
       "      <td>0.097719</td>\n",
       "      <td>0.053108</td>\n",
       "      <td>0.489655</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.015932</td>\n",
       "      <td>0.009559</td>\n",
       "      <td>0.089221</td>\n",
       "      <td>0.045673</td>\n",
       "      <td>0.525769</td>\n",
       "      <td>0.125335</td>\n",
       "      <td>0.059481</td>\n",
       "      <td>0.439734</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>0.009468</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.014728</td>\n",
       "      <td>0.130447</td>\n",
       "      <td>0.065223</td>\n",
       "      <td>0.557555</td>\n",
       "      <td>0.104147</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.399756</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.003156</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>0.017884</td>\n",
       "      <td>0.108355</td>\n",
       "      <td>0.055755</td>\n",
       "      <td>0.460772</td>\n",
       "      <td>0.128343</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.496539</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.000331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>2</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>0.005508</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.008393</td>\n",
       "      <td>0.110153</td>\n",
       "      <td>0.068190</td>\n",
       "      <td>0.568600</td>\n",
       "      <td>0.132184</td>\n",
       "      <td>0.056650</td>\n",
       "      <td>0.386061</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.005245</td>\n",
       "      <td>0.013638</td>\n",
       "      <td>0.019932</td>\n",
       "      <td>0.119595</td>\n",
       "      <td>0.066092</td>\n",
       "      <td>0.487821</td>\n",
       "      <td>0.111202</td>\n",
       "      <td>0.048258</td>\n",
       "      <td>0.466840</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.000433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6996 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      result  odds-home  odds-draw  odds-away  home-wins  home-draws  \\\n",
       "0          2   0.003670   0.003460   0.002202   0.001049    0.003146   \n",
       "1          2   0.003311   0.003311   0.002343   0.005094    0.002037   \n",
       "2          2   0.001254   0.006271   0.019859   0.007317    0.002090   \n",
       "3          0   0.002499   0.003465   0.003150   0.003150    0.003150   \n",
       "4          0   0.002715   0.003394   0.002924   0.004177    0.001044   \n",
       "...      ...        ...        ...        ...        ...         ...   \n",
       "6991       2   0.005271   0.004006   0.001792   0.004217    0.002108   \n",
       "6992       2   0.002133   0.003840   0.003946   0.003200    0.002133   \n",
       "6993       2   0.001912   0.003983   0.004780   0.003186    0.003186   \n",
       "6994       2   0.001399   0.005523   0.009468   0.002104    0.003156   \n",
       "6995       2   0.001752   0.004406   0.005508   0.006294    0.001049   \n",
       "\n",
       "      home-losses  home-goals  home-opposition-goals  home-shots  \\\n",
       "0        0.006291    0.011534               0.016777    0.143652   \n",
       "1        0.003056    0.019356               0.012225    0.192540   \n",
       "2        0.001045    0.015678               0.008362    0.168280   \n",
       "3        0.004200    0.009449               0.011549    0.107092   \n",
       "4        0.005222    0.015665               0.016709    0.130540   \n",
       "...           ...         ...                    ...         ...   \n",
       "6991     0.004217    0.010541               0.012650    0.084331   \n",
       "6992     0.005333    0.009599               0.022398    0.094925   \n",
       "6993     0.004249    0.008497               0.008497    0.129583   \n",
       "6994     0.005260    0.012624               0.014728    0.130447   \n",
       "6995     0.003147    0.010491               0.008393    0.110153   \n",
       "\n",
       "      home-shots_on_target  home-possession  home-opposition_shots  \\\n",
       "0                 0.070253         0.504354               0.122681   \n",
       "1                 0.090667         0.550114               0.070292   \n",
       "2                 0.075255         0.498567               0.077346   \n",
       "3                 0.049346         0.460917               0.139640   \n",
       "4                 0.065792         0.454280               0.159781   \n",
       "...                    ...              ...                    ...   \n",
       "6991              0.045328         0.436415               0.123335   \n",
       "6992              0.050129         0.417030               0.098125   \n",
       "6993              0.062667         0.476909               0.097719   \n",
       "6994              0.065223         0.557555               0.104147   \n",
       "6995              0.068190         0.568600               0.132184   \n",
       "\n",
       "      home-opposition_shots_on_target  home-opposition_possession  away-wins  \\\n",
       "0                            0.055573                    0.449829   0.008388   \n",
       "1                            0.036674                    0.375911   0.006112   \n",
       "2                            0.032402                    0.452578   0.002090   \n",
       "3                            0.080844                    0.494515   0.003150   \n",
       "4                            0.076236                    0.496053   0.004177   \n",
       "...                               ...                         ...        ...   \n",
       "6991                         0.063249                    0.521800   0.002108   \n",
       "6992                         0.049062                    0.553551   0.001067   \n",
       "6993                         0.053108                    0.489655   0.005311   \n",
       "6994                         0.052600                    0.399756   0.003156   \n",
       "6995                         0.056650                    0.386061   0.003147   \n",
       "\n",
       "      away-draws  away-losses  away-goals  away-opposition-goals  away-shots  \\\n",
       "0       0.002097     0.000000    0.015728               0.006291    0.168817   \n",
       "1       0.003056     0.001019    0.019356               0.008150    0.172165   \n",
       "2       0.002090     0.006271    0.009407               0.018814    0.137968   \n",
       "3       0.002100     0.005250    0.009449               0.016799    0.121791   \n",
       "4       0.003133     0.003133    0.013576               0.016709    0.107565   \n",
       "...          ...          ...         ...                    ...         ...   \n",
       "6991    0.004217     0.004217    0.010541               0.022137    0.125443   \n",
       "6992    0.004266     0.005333    0.007466               0.022398    0.110923   \n",
       "6993    0.002124     0.003186    0.015932               0.009559    0.089221   \n",
       "6994    0.003156     0.004208    0.011572               0.017884    0.108355   \n",
       "6995    0.002098     0.005245    0.013638               0.019932    0.119595   \n",
       "\n",
       "      away-shots_on_target  away-possession  away-opposition_shots  \\\n",
       "0                 0.081787         0.505402               0.075496   \n",
       "1                 0.096779         0.558263               0.115116   \n",
       "2                 0.064803         0.443171               0.145285   \n",
       "3                 0.056696         0.430469               0.137540   \n",
       "4                 0.053260         0.456369               0.144117   \n",
       "...                    ...              ...                    ...   \n",
       "6991              0.061140         0.410061               0.118064   \n",
       "6992              0.058661         0.447960               0.067194   \n",
       "6993              0.045673         0.525769               0.125335   \n",
       "6994              0.055755         0.460772               0.128343   \n",
       "6995              0.066092         0.487821               0.111202   \n",
       "\n",
       "      away-opposition_shots_on_target  away-opposition_possession  \\\n",
       "0                            0.031457                    0.448780   \n",
       "1                            0.059086                    0.368780   \n",
       "2                            0.074210                    0.507974   \n",
       "3                            0.067195                    0.524962   \n",
       "4                            0.079369                    0.493965   \n",
       "...                               ...                         ...   \n",
       "6991                         0.068519                    0.549208   \n",
       "6992                         0.027731                    0.522620   \n",
       "6993                         0.059481                    0.439734   \n",
       "6994                         0.056807                    0.496539   \n",
       "6995                         0.048258                    0.466840   \n",
       "\n",
       "      home_shot_accuracy  home_shot_efficiency  home_opposition_shot_accuracy  \\\n",
       "0               0.000513              0.000172                       0.000475   \n",
       "1               0.000480              0.000217                       0.000532   \n",
       "2               0.000467              0.000218                       0.000438   \n",
       "3               0.000484              0.000201                       0.000608   \n",
       "4               0.000526              0.000249                       0.000498   \n",
       "...                  ...                   ...                            ...   \n",
       "6991            0.000567              0.000245                       0.000541   \n",
       "6992            0.000563              0.000204                       0.000533   \n",
       "6993            0.000514              0.000144                       0.000577   \n",
       "6994            0.000526              0.000204                       0.000531   \n",
       "6995            0.000649              0.000161                       0.000450   \n",
       "\n",
       "      home_opposition_shot_efficiency  away_shot_accuracy  \\\n",
       "0                            0.000317            0.000508   \n",
       "1                            0.000340            0.000573   \n",
       "2                            0.000270            0.000491   \n",
       "3                            0.000150            0.000489   \n",
       "4                            0.000229            0.000517   \n",
       "...                               ...                 ...   \n",
       "6991                         0.000211            0.000514   \n",
       "6992                         0.000487            0.000564   \n",
       "6993                         0.000170            0.000544   \n",
       "6994                         0.000295            0.000541   \n",
       "6995                         0.000155            0.000580   \n",
       "\n",
       "      away_shot_efficiency  away_opposition_shot_accuracy  \\\n",
       "0                 0.000202                       0.000437   \n",
       "1                 0.000204                       0.000523   \n",
       "2                 0.000152                       0.000534   \n",
       "3                 0.000175                       0.000513   \n",
       "4                 0.000266                       0.000575   \n",
       "...                    ...                            ...   \n",
       "6991              0.000182                       0.000612   \n",
       "6992              0.000136                       0.000440   \n",
       "6993              0.000371                       0.000504   \n",
       "6994              0.000218                       0.000466   \n",
       "6995              0.000216                       0.000455   \n",
       "\n",
       "      away_opposition_shot_efficiency  \n",
       "0                            0.000210  \n",
       "1                            0.000141  \n",
       "2                            0.000265  \n",
       "3                            0.000262  \n",
       "4                            0.000220  \n",
       "...                               ...  \n",
       "6991                         0.000341  \n",
       "6992                         0.000861  \n",
       "6993                         0.000171  \n",
       "6994                         0.000331  \n",
       "6995                         0.000433  \n",
       "\n",
       "[6996 rows x 34 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Backup:\n",
    "\n",
    "## I. First Attempt:\n",
    "\n",
    "### Model01:\n",
    "\n",
    "model01 = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "Test Score: 0.9784467357591566\n",
    "Test Accuracy: 0.5393474\n",
    "### Model02:\n",
    "\n",
    "model02 = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(32, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "Test Score: 0.9985063346949491\n",
    "Test Accuracy: 0.5113636\n",
    "### Model03:\n",
    "\n",
    "model03 = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(20, activation='relu'),\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "Test Score: 0.9796081943945452\n",
    "Test Accuracy: 0.5255682\n",
    "### Model04:\n",
    "\n",
    "model04 = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(50, activation='relu'),\n",
    "  layers.Dense(30, activation='relu'),\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "Test Score: 0.970786578314645\n",
    "Test Accuracy: 0.5228571\n",
    "### Model05:\n",
    "\n",
    "model05 = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(66, activation='relu'),\n",
    "  layers.Dense(55, activation='relu'),\n",
    "  layers.Dense(44, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(22, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "Test Score: 1.013617993082319\n",
    "Test Accuracy: 0.52\n",
    "\n",
    "\n",
    "## II. Layer & Neuron Variantion:\n",
    "### 1 Hidden Layer:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01-H1-H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H1-H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H1-H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H1-H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H1-H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01-H1-M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H1-M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H1-M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H1-M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H1-M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01-H1-L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H1-L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H1-L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H1-L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H1-L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "### 2 Hidden Layers:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01-H2-H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H2-H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H2-H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H2-H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H2-H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01-H2-M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H2-M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H2-M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H2-M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H2-M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01-H2-L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H2-L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H2-L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H2-L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H2-L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "### 3 Hidden Layers:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01-H3-H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H3-H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H3-H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H3-H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H3-H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01-H3-M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H3-M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H3-M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H3-M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H3-M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01-H3-L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H3-L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H3-L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H3-L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H3-L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Funnel Architecture:\n",
    "##### Model01:\n",
    "\n",
    "model01-H3-F = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H3-F = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(18, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H3-F = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(22, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H3-F = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(20, activation='relu'),\n",
    "  layers.Dense(15, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H3-F = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(19, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "### 4 Hidden Layers:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01-H4-H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H4-H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H4-H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H4-H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H4-H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01-H4-M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H4-M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H4-M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H4-M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H4-M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01-H4-L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H4-L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H4-L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H4-L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H4-L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Funnel Architecture:\n",
    "##### Model01:\n",
    "\n",
    "model01-H4-F = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02-H4-F = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(18, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03-H4-F = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(22, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04-H4-F = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(20, activation='relu'),\n",
    "  layers.Dense(15, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05-H4-F = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(19, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HtHfSEgO6sUN",
    "outputId": "b7fd6a48-30f1-470e-f92b-118a7046a09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_251\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1152 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1153 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1154 (Dense)           (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 406\n",
      "Trainable params: 406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.3879,  loss:1.0898,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4265,  val_acc:0.4941,  val_loss:1.0757,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4015,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5313,  loss:0.9741,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4546,  val_acc:0.5219,  val_loss:0.9674,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4304,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5314,  loss:0.9732,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4549,  val_acc:0.5237,  val_loss:0.9664,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4298,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5318,  loss:0.9729,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4556,  val_acc:0.5291,  val_loss:0.9659,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851608.0000,  val_mean_squared_error:1.4289,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5302,  loss:0.9729,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4551,  val_acc:0.5259,  val_loss:0.9662,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4303,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5316,  loss:0.9722,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4547,  val_acc:0.5269,  val_loss:0.9658,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4298,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5320,  loss:0.9720,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4550,  val_acc:0.5253,  val_loss:0.9661,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4303,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5313,  loss:0.9718,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961736.0000,  mean_squared_error:1.4551,  val_acc:0.5283,  val_loss:0.9656,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4295,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5316,  loss:0.9716,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961736.0000,  mean_squared_error:1.4553,  val_acc:0.5264,  val_loss:0.9659,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4300,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5320,  loss:0.9714,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4551,  val_acc:0.5275,  val_loss:0.9657,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4296,  \n",
      "............................................................................................Model: \"sequential_252\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1155 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1156 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1157 (Dense)           (None, 3)                 66        \n",
      "=================================================================\n",
      "Total params: 990\n",
      "Trainable params: 990\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.4527,  loss:1.0876,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4333,  val_acc:0.4415,  val_loss:1.0897,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369112.0000,  val_mean_squared_error:1.3754,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5260,  loss:0.9859,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4554,  val_acc:0.5237,  val_loss:0.9777,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369096.0000,  val_mean_squared_error:1.3996,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5313,  loss:0.9769,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4576,  val_acc:0.5371,  val_loss:0.9692,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369096.0000,  val_mean_squared_error:1.4018,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5339,  loss:0.9716,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4593,  val_acc:0.5292,  val_loss:0.9690,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369112.0000,  val_mean_squared_error:1.4041,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5376,  loss:0.9683,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4597,  val_acc:0.5348,  val_loss:0.9649,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4032,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5372,  loss:0.9654,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595464.0000,  mean_squared_error:1.4607,  val_acc:0.5332,  val_loss:0.9663,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4045,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5390,  loss:0.9641,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4614,  val_acc:0.5340,  val_loss:0.9680,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4056,  \n",
      "....Model: \"sequential_253\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1158 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1159 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1160 (Dense)           (None, 3)                 90        \n",
      "=================================================================\n",
      "Total params: 1,830\n",
      "Trainable params: 1,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, acc:0.3235,  loss:1.0921,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4331,  val_acc:0.4502,  val_loss:1.0852,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4137,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5311,  loss:0.9780,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4597,  val_acc:0.5237,  val_loss:0.9855,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4394,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5351,  loss:0.9714,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4621,  val_acc:0.5300,  val_loss:0.9746,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472864.0000,  val_mean_squared_error:1.4417,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5368,  loss:0.9678,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4624,  val_acc:0.5332,  val_loss:0.9693,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4434,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5402,  loss:0.9649,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4630,  val_acc:0.5387,  val_loss:0.9655,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4438,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5426,  loss:0.9632,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4640,  val_acc:0.5340,  val_loss:0.9633,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472864.0000,  val_mean_squared_error:1.4442,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5422,  loss:0.9619,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4639,  val_acc:0.5355,  val_loss:0.9620,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4438,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5436,  loss:0.9607,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4634,  val_acc:0.5348,  val_loss:0.9617,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4440,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5420,  loss:0.9598,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4649,  val_acc:0.5348,  val_loss:0.9619,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4447,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5430,  loss:0.9587,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4645,  val_acc:0.5355,  val_loss:0.9620,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472856.0000,  val_mean_squared_error:1.4443,  \n",
      "..............................Model: \"sequential_254\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1161 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1162 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1163 (Dense)           (None, 3)                 78        \n",
      "=================================================================\n",
      "Total params: 1,378\n",
      "Trainable params: 1,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.3650,  loss:1.0956,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4186,  val_acc:0.4802,  val_loss:1.0842,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4444,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5159,  loss:0.9925,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4424,  val_acc:0.5381,  val_loss:0.9695,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4695,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5199,  loss:0.9841,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4439,  val_acc:0.5429,  val_loss:0.9654,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4691,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5246,  loss:0.9789,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4452,  val_acc:0.5476,  val_loss:0.9601,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4718,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5276,  loss:0.9755,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4466,  val_acc:0.5516,  val_loss:0.9586,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4731,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5300,  loss:0.9729,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4467,  val_acc:0.5516,  val_loss:0.9572,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4745,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5278,  loss:0.9709,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4468,  val_acc:0.5500,  val_loss:0.9570,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4739,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5294,  loss:0.9687,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777024.0000,  mean_squared_error:1.4478,  val_acc:0.5476,  val_loss:0.9562,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4751,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5302,  loss:0.9668,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4475,  val_acc:0.5500,  val_loss:0.9556,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4758,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5308,  loss:0.9649,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4486,  val_acc:0.5603,  val_loss:0.9558,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4775,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5336,  loss:0.9638,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4477,  val_acc:0.5532,  val_loss:0.9557,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4762,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5310,  loss:0.9642,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4502,  val_acc:0.5548,  val_loss:0.9559,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4765,  \n",
      "................................................Model: \"sequential_255\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1164 (Dense)           (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dense_1165 (Dense)           (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dense_1166 (Dense)           (None, 3)                 102       \n",
      "=================================================================\n",
      "Total params: 2,346\n",
      "Trainable params: 2,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.4708,  loss:1.0874,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4498,  val_acc:0.4317,  val_loss:1.0835,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3600,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5282,  loss:0.9777,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4764,  val_acc:0.5016,  val_loss:1.0178,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3867,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5375,  loss:0.9664,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4774,  val_acc:0.5087,  val_loss:1.0146,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3883,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5415,  loss:0.9609,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4788,  val_acc:0.5063,  val_loss:1.0138,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3886,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5435,  loss:0.9579,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4800,  val_acc:0.5087,  val_loss:1.0142,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3899,  \n",
      ".......Model: \"sequential_256\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1167 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1168 (Dense)           (None, 9)                 126       \n",
      "_________________________________________________________________\n",
      "dense_1169 (Dense)           (None, 3)                 30        \n",
      "=================================================================\n",
      "Total params: 338\n",
      "Trainable params: 338\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2747,  loss:1.1054,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4267,  val_acc:0.3525,  val_loss:1.0945,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4017,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5301,  loss:0.9745,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4546,  val_acc:0.5293,  val_loss:0.9679,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4285,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5303,  loss:0.9733,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4546,  val_acc:0.5256,  val_loss:0.9677,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4290,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5304,  loss:0.9726,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961736.0000,  mean_squared_error:1.4554,  val_acc:0.5288,  val_loss:0.9673,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4288,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5311,  loss:0.9721,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961704.0000,  mean_squared_error:1.4550,  val_acc:0.5291,  val_loss:0.9672,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4291,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5308,  loss:0.9718,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4549,  val_acc:0.5301,  val_loss:0.9671,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4291,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5307,  loss:0.9716,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4555,  val_acc:0.5275,  val_loss:0.9674,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4306,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5310,  loss:0.9713,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4554,  val_acc:0.5309,  val_loss:0.9669,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4300,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5308,  loss:0.9711,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961704.0000,  mean_squared_error:1.4550,  val_acc:0.5301,  val_loss:0.9671,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4297,  \n",
      "...................................................................Model: \"sequential_257\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1170 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1171 (Dense)           (None, 12)                264       \n",
      "_________________________________________________________________\n",
      "dense_1172 (Dense)           (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 765\n",
      "Trainable params: 765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, acc:0.2493,  loss:1.1293,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4324,  val_acc:0.2543,  val_loss:1.1181,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.3740,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5260,  loss:0.9886,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4553,  val_acc:0.5134,  val_loss:0.9831,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369088.0000,  val_mean_squared_error:1.3985,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5311,  loss:0.9773,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4582,  val_acc:0.5308,  val_loss:0.9724,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4019,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5345,  loss:0.9717,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4593,  val_acc:0.5332,  val_loss:0.9682,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369096.0000,  val_mean_squared_error:1.4030,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5358,  loss:0.9681,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4601,  val_acc:0.5308,  val_loss:0.9670,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369096.0000,  val_mean_squared_error:1.4040,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5400,  loss:0.9659,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4605,  val_acc:0.5332,  val_loss:0.9661,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4045,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5386,  loss:0.9645,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4613,  val_acc:0.5363,  val_loss:0.9664,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4053,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5392,  loss:0.9639,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4612,  val_acc:0.5348,  val_loss:0.9655,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369096.0000,  val_mean_squared_error:1.4050,  \n",
      ".....Model: \"sequential_258\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1173 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1174 (Dense)           (None, 14)                420       \n",
      "_________________________________________________________________\n",
      "dense_1175 (Dense)           (None, 3)                 45        \n",
      "=================================================================\n",
      "Total params: 1,335\n",
      "Trainable params: 1,335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.4610,  loss:1.0904,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4329,  val_acc:0.4573,  val_loss:1.0856,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4135,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5291,  loss:0.9802,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4588,  val_acc:0.5261,  val_loss:0.9875,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4398,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5358,  loss:0.9730,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4622,  val_acc:0.5269,  val_loss:0.9763,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4417,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5386,  loss:0.9685,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4611,  val_acc:0.5355,  val_loss:0.9709,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4424,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5422,  loss:0.9653,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4625,  val_acc:0.5387,  val_loss:0.9675,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4429,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5408,  loss:0.9634,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4627,  val_acc:0.5355,  val_loss:0.9661,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4426,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5418,  loss:0.9628,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4627,  val_acc:0.5355,  val_loss:0.9653,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4430,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5428,  loss:0.9614,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4635,  val_acc:0.5340,  val_loss:0.9643,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4438,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5420,  loss:0.9611,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4645,  val_acc:0.5332,  val_loss:0.9641,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4451,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5424,  loss:0.9599,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4637,  val_acc:0.5332,  val_loss:0.9635,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4445,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5457,  loss:0.9599,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4650,  val_acc:0.5324,  val_loss:0.9633,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4451,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5451,  loss:0.9589,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4652,  val_acc:0.5332,  val_loss:0.9637,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4457,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................Model: \"sequential_259\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1176 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1177 (Dense)           (None, 13)                338       \n",
      "_________________________________________________________________\n",
      "dense_1178 (Dense)           (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 1,030\n",
      "Trainable params: 1,030\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2520,  loss:1.1324,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4196,  val_acc:0.2381,  val_loss:1.1212,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4447,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5137,  loss:0.9995,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4409,  val_acc:0.5373,  val_loss:0.9758,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4664,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5155,  loss:0.9932,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4420,  val_acc:0.5437,  val_loss:0.9675,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4688,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5214,  loss:0.9886,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4436,  val_acc:0.5468,  val_loss:0.9623,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4700,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5244,  loss:0.9848,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4434,  val_acc:0.5429,  val_loss:0.9594,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4703,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5240,  loss:0.9821,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4446,  val_acc:0.5476,  val_loss:0.9567,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4716,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5250,  loss:0.9799,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777024.0000,  mean_squared_error:1.4442,  val_acc:0.5476,  val_loss:0.9559,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4716,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5272,  loss:0.9782,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4448,  val_acc:0.5452,  val_loss:0.9549,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4722,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5268,  loss:0.9764,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4455,  val_acc:0.5444,  val_loss:0.9539,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708944.0000,  val_mean_squared_error:1.4729,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5282,  loss:0.9752,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4463,  val_acc:0.5444,  val_loss:0.9535,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4731,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5298,  loss:0.9737,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4458,  val_acc:0.5484,  val_loss:0.9528,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4741,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5308,  loss:0.9726,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4461,  val_acc:0.5484,  val_loss:0.9531,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4734,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5296,  loss:0.9717,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4469,  val_acc:0.5508,  val_loss:0.9525,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4743,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5292,  loss:0.9712,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777024.0000,  mean_squared_error:1.4476,  val_acc:0.5516,  val_loss:0.9525,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4745,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, acc:0.5298,  loss:0.9702,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4468,  val_acc:0.5532,  val_loss:0.9526,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4746,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, acc:0.5322,  loss:0.9697,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4465,  val_acc:0.5532,  val_loss:0.9531,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4743,  \n",
      "..........................................................Model: \"sequential_260\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1179 (Dense)           (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dense_1180 (Dense)           (None, 16)                544       \n",
      "_________________________________________________________________\n",
      "dense_1181 (Dense)           (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 1,717\n",
      "Trainable params: 1,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2905,  loss:1.1245,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4523,  val_acc:0.2802,  val_loss:1.1207,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3618,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5278,  loss:0.9816,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4741,  val_acc:0.4992,  val_loss:1.0204,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3843,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5328,  loss:0.9726,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4769,  val_acc:0.5063,  val_loss:1.0164,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3873,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5346,  loss:0.9669,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4786,  val_acc:0.5087,  val_loss:1.0146,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3880,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5401,  loss:0.9632,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4782,  val_acc:0.5119,  val_loss:1.0141,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3889,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5435,  loss:0.9604,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4797,  val_acc:0.5103,  val_loss:1.0145,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386208.0000,  val_mean_squared_error:1.3903,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5425,  loss:0.9587,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4795,  val_acc:0.5095,  val_loss:1.0134,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3901,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5455,  loss:0.9571,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4809,  val_acc:0.5095,  val_loss:1.0138,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3910,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5469,  loss:0.9557,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836064.0000,  mean_squared_error:1.4811,  val_acc:0.5095,  val_loss:1.0134,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3914,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5477,  loss:0.9546,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836064.0000,  mean_squared_error:1.4809,  val_acc:0.5103,  val_loss:1.0119,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3908,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5473,  loss:0.9536,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4809,  val_acc:0.5095,  val_loss:1.0106,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3904,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5483,  loss:0.9522,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4816,  val_acc:0.5079,  val_loss:1.0099,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3904,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5492,  loss:0.9514,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4821,  val_acc:0.5071,  val_loss:1.0096,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3907,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5483,  loss:0.9511,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4829,  val_acc:0.5071,  val_loss:1.0102,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386208.0000,  val_mean_squared_error:1.3918,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, acc:0.5459,  loss:0.9509,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4822,  val_acc:0.5071,  val_loss:1.0103,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3922,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, acc:0.5475,  loss:0.9499,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4820,  val_acc:0.5056,  val_loss:1.0098,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3919,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, acc:0.5498,  loss:0.9499,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836064.0000,  mean_squared_error:1.4811,  val_acc:0.5079,  val_loss:1.0089,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3913,  \n",
      ".................................................................................................Model: \"sequential_261\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1182 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1183 (Dense)           (None, 4)                 56        \n",
      "_________________________________________________________________\n",
      "dense_1184 (Dense)           (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 253\n",
      "Trainable params: 253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.3645,  loss:1.0992,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961704.0000,  mean_squared_error:1.4261,  val_acc:0.4490,  val_loss:1.0956,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851608.0000,  val_mean_squared_error:1.4010,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5277,  loss:0.9840,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961704.0000,  mean_squared_error:1.4475,  val_acc:0.5253,  val_loss:0.9822,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4234,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5291,  loss:0.9781,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4525,  val_acc:0.5283,  val_loss:0.9707,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4266,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5297,  loss:0.9765,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4530,  val_acc:0.5216,  val_loss:0.9696,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4281,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5301,  loss:0.9760,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4530,  val_acc:0.5237,  val_loss:0.9691,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4278,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5304,  loss:0.9753,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4537,  val_acc:0.5240,  val_loss:0.9690,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851608.0000,  val_mean_squared_error:1.4281,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5301,  loss:0.9752,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4535,  val_acc:0.5243,  val_loss:0.9687,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851608.0000,  val_mean_squared_error:1.4289,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5306,  loss:0.9750,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4542,  val_acc:0.5243,  val_loss:0.9682,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4286,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5305,  loss:0.9748,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4543,  val_acc:0.5261,  val_loss:0.9681,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4289,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5299,  loss:0.9746,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4542,  val_acc:0.5261,  val_loss:0.9679,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4285,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5304,  loss:0.9746,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4544,  val_acc:0.5267,  val_loss:0.9677,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4288,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5309,  loss:0.9747,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4551,  val_acc:0.5256,  val_loss:0.9679,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4291,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5305,  loss:0.9743,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961728.0000,  mean_squared_error:1.4545,  val_acc:0.5277,  val_loss:0.9677,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4286,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5307,  loss:0.9743,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961736.0000,  mean_squared_error:1.4548,  val_acc:0.5267,  val_loss:0.9679,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4291,  \n",
      ".........Model: \"sequential_262\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1185 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1186 (Dense)           (None, 5)                 110       \n",
      "_________________________________________________________________\n",
      "dense_1187 (Dense)           (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 590\n",
      "Trainable params: 590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.4640,  loss:1.0748,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4366,  val_acc:0.4415,  val_loss:1.0766,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.3781,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5220,  loss:0.9882,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4576,  val_acc:0.5197,  val_loss:0.9828,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4011,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5305,  loss:0.9761,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4582,  val_acc:0.5324,  val_loss:0.9700,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4021,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5315,  loss:0.9702,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4600,  val_acc:0.5324,  val_loss:0.9676,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369088.0000,  val_mean_squared_error:1.4037,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5366,  loss:0.9663,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4600,  val_acc:0.5300,  val_loss:0.9668,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4051,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5382,  loss:0.9639,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4610,  val_acc:0.5332,  val_loss:0.9652,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4051,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5392,  loss:0.9628,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4615,  val_acc:0.5332,  val_loss:0.9660,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4059,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5394,  loss:0.9621,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4623,  val_acc:0.5340,  val_loss:0.9665,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4062,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Model: \"sequential_263\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1188 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1189 (Dense)           (None, 7)                 210       \n",
      "_________________________________________________________________\n",
      "dense_1190 (Dense)           (None, 3)                 24        \n",
      "=================================================================\n",
      "Total params: 1,104\n",
      "Trainable params: 1,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2502,  loss:1.0992,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4332,  val_acc:0.2559,  val_loss:1.0958,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4136,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5270,  loss:0.9835,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4584,  val_acc:0.5221,  val_loss:0.9921,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4382,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5362,  loss:0.9744,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4607,  val_acc:0.5261,  val_loss:0.9777,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4416,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5355,  loss:0.9711,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4606,  val_acc:0.5332,  val_loss:0.9713,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4414,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5392,  loss:0.9682,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4619,  val_acc:0.5387,  val_loss:0.9671,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4421,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5402,  loss:0.9664,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4619,  val_acc:0.5395,  val_loss:0.9643,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4424,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5414,  loss:0.9650,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4622,  val_acc:0.5363,  val_loss:0.9624,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4425,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5400,  loss:0.9642,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4628,  val_acc:0.5371,  val_loss:0.9611,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472864.0000,  val_mean_squared_error:1.4432,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5404,  loss:0.9636,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4632,  val_acc:0.5371,  val_loss:0.9603,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4435,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5404,  loss:0.9632,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4638,  val_acc:0.5355,  val_loss:0.9598,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472856.0000,  val_mean_squared_error:1.4438,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5400,  loss:0.9628,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4628,  val_acc:0.5371,  val_loss:0.9594,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4435,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5392,  loss:0.9623,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4628,  val_acc:0.5363,  val_loss:0.9589,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4439,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5396,  loss:0.9618,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4632,  val_acc:0.5371,  val_loss:0.9589,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4436,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5398,  loss:0.9617,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4637,  val_acc:0.5348,  val_loss:0.9585,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472856.0000,  val_mean_squared_error:1.4443,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, acc:0.5396,  loss:0.9611,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4636,  val_acc:0.5348,  val_loss:0.9583,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4447,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, acc:0.5394,  loss:0.9609,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4637,  val_acc:0.5348,  val_loss:0.9581,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4443,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, acc:0.5396,  loss:0.9608,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4638,  val_acc:0.5363,  val_loss:0.9581,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4444,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, acc:0.5380,  loss:0.9607,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4637,  val_acc:0.5348,  val_loss:0.9581,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4444,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, acc:0.5400,  loss:0.9604,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4640,  val_acc:0.5355,  val_loss:0.9581,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4449,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 1900, acc:0.5400,  loss:0.9601,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4634,  val_acc:0.5340,  val_loss:0.9582,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472856.0000,  val_mean_squared_error:1.4446,  \n",
      ".............................................................Model: \"sequential_264\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1191 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1192 (Dense)           (None, 6)                 156       \n",
      "_________________________________________________________________\n",
      "dense_1193 (Dense)           (None, 3)                 21        \n",
      "=================================================================\n",
      "Total params: 827\n",
      "Trainable params: 827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2780,  loss:1.1020,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4186,  val_acc:0.3841,  val_loss:1.0958,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4442,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5133,  loss:0.9993,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4393,  val_acc:0.5413,  val_loss:0.9766,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4645,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5145,  loss:0.9921,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4425,  val_acc:0.5405,  val_loss:0.9677,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4692,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5201,  loss:0.9880,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4440,  val_acc:0.5429,  val_loss:0.9635,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4700,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5203,  loss:0.9844,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777024.0000,  mean_squared_error:1.4441,  val_acc:0.5452,  val_loss:0.9609,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4707,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5244,  loss:0.9816,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4451,  val_acc:0.5468,  val_loss:0.9588,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4722,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5304,  loss:0.9793,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777024.0000,  mean_squared_error:1.4441,  val_acc:0.5460,  val_loss:0.9575,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4721,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5298,  loss:0.9772,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4453,  val_acc:0.5484,  val_loss:0.9568,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4725,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5298,  loss:0.9759,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4454,  val_acc:0.5468,  val_loss:0.9566,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4727,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5304,  loss:0.9743,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4455,  val_acc:0.5452,  val_loss:0.9559,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4735,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5316,  loss:0.9729,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4464,  val_acc:0.5460,  val_loss:0.9559,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4735,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5306,  loss:0.9718,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4465,  val_acc:0.5500,  val_loss:0.9556,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4737,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5308,  loss:0.9706,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4466,  val_acc:0.5508,  val_loss:0.9554,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4741,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5304,  loss:0.9700,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4476,  val_acc:0.5484,  val_loss:0.9548,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4750,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, acc:0.5328,  loss:0.9689,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4469,  val_acc:0.5516,  val_loss:0.9550,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4749,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, acc:0.5322,  loss:0.9682,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777024.0000,  mean_squared_error:1.4472,  val_acc:0.5524,  val_loss:0.9550,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4752,  \n",
      "...................................................................Model: \"sequential_265\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1194 (Dense)           (None, 33)                1122      \n",
      "_________________________________________________________________\n",
      "dense_1195 (Dense)           (None, 8)                 272       \n",
      "_________________________________________________________________\n",
      "dense_1196 (Dense)           (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 1,421\n",
      "Trainable params: 1,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0, acc:0.2905,  loss:1.1150,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4488,  val_acc:0.2802,  val_loss:1.1077,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3587,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5260,  loss:0.9845,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4732,  val_acc:0.4968,  val_loss:1.0225,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3840,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5310,  loss:0.9772,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4758,  val_acc:0.4992,  val_loss:1.0182,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3859,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5373,  loss:0.9723,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4770,  val_acc:0.5071,  val_loss:1.0171,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3869,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5393,  loss:0.9685,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4771,  val_acc:0.5095,  val_loss:1.0161,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3873,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5395,  loss:0.9658,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836064.0000,  mean_squared_error:1.4778,  val_acc:0.5103,  val_loss:1.0161,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3883,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5403,  loss:0.9643,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4773,  val_acc:0.5063,  val_loss:1.0148,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3876,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5413,  loss:0.9626,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4785,  val_acc:0.5071,  val_loss:1.0147,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386232.0000,  val_mean_squared_error:1.3883,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5425,  loss:0.9614,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4799,  val_acc:0.5103,  val_loss:1.0149,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3892,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5425,  loss:0.9601,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4795,  val_acc:0.5079,  val_loss:1.0143,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3893,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5443,  loss:0.9592,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4797,  val_acc:0.5056,  val_loss:1.0130,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3885,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5431,  loss:0.9587,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4796,  val_acc:0.5032,  val_loss:1.0124,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3886,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5441,  loss:0.9575,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4797,  val_acc:0.5079,  val_loss:1.0126,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3894,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5455,  loss:0.9568,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4801,  val_acc:0.5079,  val_loss:1.0129,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3903,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, acc:0.5465,  loss:0.9563,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4800,  val_acc:0.5040,  val_loss:1.0116,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3894,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, acc:0.5477,  loss:0.9557,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4799,  val_acc:0.5040,  val_loss:1.0114,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3895,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, acc:0.5477,  loss:0.9552,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4806,  val_acc:0.5063,  val_loss:1.0113,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3898,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, acc:0.5494,  loss:0.9548,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4808,  val_acc:0.5063,  val_loss:1.0111,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3901,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, acc:0.5494,  loss:0.9546,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4808,  val_acc:0.5071,  val_loss:1.0109,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3902,  \n",
      "....................................................................................................\n",
      "Epoch: 1900, acc:0.5461,  loss:0.9545,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4800,  val_acc:0.5024,  val_loss:1.0101,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3895,  \n",
      "....................................................................................................\n",
      "Epoch: 2000, acc:0.5490,  loss:0.9541,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4816,  val_acc:0.5071,  val_loss:1.0106,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3904,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 2100, acc:0.5490,  loss:0.9537,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4809,  val_acc:0.5063,  val_loss:1.0105,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3905,  \n",
      "....................................................................................................\n",
      "Epoch: 2200, acc:0.5490,  loss:0.9537,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4817,  val_acc:0.5071,  val_loss:1.0102,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3904,  \n",
      "....................................................................................................\n",
      "Epoch: 2300, acc:0.5490,  loss:0.9534,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836080.0000,  mean_squared_error:1.4810,  val_acc:0.5071,  val_loss:1.0102,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3906,  \n",
      "....................................................................................................\n",
      "Epoch: 2400, acc:0.5494,  loss:0.9534,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4814,  val_acc:0.5079,  val_loss:1.0105,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3910,  \n",
      "....................................................................................................\n",
      "Epoch: 2500, acc:0.5490,  loss:0.9532,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4817,  val_acc:0.5048,  val_loss:1.0099,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3907,  \n",
      "....................................................................................................\n",
      "Epoch: 2600, acc:0.5496,  loss:0.9530,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4809,  val_acc:0.5063,  val_loss:1.0098,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386224.0000,  val_mean_squared_error:1.3906,  \n",
      "....................................................................................................\n",
      "Epoch: 2700, acc:0.5485,  loss:0.9530,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4809,  val_acc:0.5056,  val_loss:1.0097,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3907,  \n",
      "....................................................................................................\n",
      "Epoch: 2800, acc:0.5490,  loss:0.9528,  mean_absolute_error:1.0416,  mean_absolute_percentage_error:96836072.0000,  mean_squared_error:1.4812,  val_acc:0.5056,  val_loss:1.0092,  val_mean_absolute_error:1.0050,  val_mean_absolute_percentage_error:93386216.0000,  val_mean_squared_error:1.3902,  \n",
      "..................................................................................Model: \"sequential_266\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1197 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1198 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1199 (Dense)           (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_1200 (Dense)           (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 588\n",
      "Trainable params: 588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.4019,  loss:1.0859,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4269,  val_acc:0.4517,  val_loss:1.0803,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4025,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5318,  loss:0.9739,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4549,  val_acc:0.5229,  val_loss:0.9681,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4294,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5326,  loss:0.9735,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4555,  val_acc:0.5224,  val_loss:0.9681,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4303,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5324,  loss:0.9723,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961736.0000,  mean_squared_error:1.4546,  val_acc:0.5253,  val_loss:0.9675,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4301,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5330,  loss:0.9714,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4557,  val_acc:0.5272,  val_loss:0.9672,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4292,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5325,  loss:0.9709,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4552,  val_acc:0.5243,  val_loss:0.9675,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4302,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5332,  loss:0.9710,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4553,  val_acc:0.5253,  val_loss:0.9683,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851624.0000,  val_mean_squared_error:1.4312,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5326,  loss:0.9704,  mean_absolute_error:1.0331,  mean_absolute_percentage_error:94961720.0000,  mean_squared_error:1.4554,  val_acc:0.5275,  val_loss:0.9671,  val_mean_absolute_error:1.0213,  val_mean_absolute_percentage_error:96851632.0000,  val_mean_squared_error:1.4292,  \n",
      "...............................................................................................Model: \"sequential_267\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1201 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1202 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1203 (Dense)           (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_1204 (Dense)           (None, 3)                 66        \n",
      "=================================================================\n",
      "Total params: 1,452\n",
      "Trainable params: 1,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2868,  loss:1.1046,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4329,  val_acc:0.3041,  val_loss:1.0950,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369088.0000,  val_mean_squared_error:1.3744,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5281,  loss:0.9800,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4587,  val_acc:0.5300,  val_loss:0.9690,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369088.0000,  val_mean_squared_error:1.3994,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5329,  loss:0.9720,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4614,  val_acc:0.5324,  val_loss:0.9685,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4048,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5353,  loss:0.9674,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4623,  val_acc:0.5355,  val_loss:0.9650,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4046,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5392,  loss:0.9647,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4615,  val_acc:0.5387,  val_loss:0.9634,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369112.0000,  val_mean_squared_error:1.4047,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5358,  loss:0.9631,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4621,  val_acc:0.5363,  val_loss:0.9628,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369088.0000,  val_mean_squared_error:1.4046,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5400,  loss:0.9624,  mean_absolute_error:1.0350,  mean_absolute_percentage_error:95595456.0000,  mean_squared_error:1.4614,  val_acc:0.5363,  val_loss:0.9645,  val_mean_absolute_error:1.0068,  val_mean_absolute_percentage_error:101369104.0000,  val_mean_squared_error:1.4061,  \n",
      "...............................................Model: \"sequential_268\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1205 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1206 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1207 (Dense)           (None, 29)                870       \n",
      "_________________________________________________________________\n",
      "dense_1208 (Dense)           (None, 3)                 90        \n",
      "=================================================================\n",
      "Total params: 2,700\n",
      "Trainable params: 2,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2500,  loss:1.1138,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4333,  val_acc:0.2472,  val_loss:1.1012,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472864.0000,  val_mean_squared_error:1.4133,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5335,  loss:0.9741,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4605,  val_acc:0.5276,  val_loss:0.9802,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4414,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5358,  loss:0.9686,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4616,  val_acc:0.5355,  val_loss:0.9723,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472856.0000,  val_mean_squared_error:1.4428,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5392,  loss:0.9654,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4634,  val_acc:0.5371,  val_loss:0.9674,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472840.0000,  val_mean_squared_error:1.4428,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5400,  loss:0.9631,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4629,  val_acc:0.5332,  val_loss:0.9641,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4444,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5414,  loss:0.9615,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4640,  val_acc:0.5308,  val_loss:0.9628,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4439,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5414,  loss:0.9599,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4645,  val_acc:0.5340,  val_loss:0.9620,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472848.0000,  val_mean_squared_error:1.4440,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5424,  loss:0.9582,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200432.0000,  mean_squared_error:1.4651,  val_acc:0.5300,  val_loss:0.9619,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472856.0000,  val_mean_squared_error:1.4454,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5436,  loss:0.9568,  mean_absolute_error:1.0358,  mean_absolute_percentage_error:95200440.0000,  mean_squared_error:1.4645,  val_acc:0.5292,  val_loss:0.9622,  val_mean_absolute_error:1.0255,  val_mean_absolute_percentage_error:98472832.0000,  val_mean_squared_error:1.4455,  \n",
      "..............Model: \"sequential_269\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1209 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1210 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1211 (Dense)           (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dense_1212 (Dense)           (None, 3)                 78        \n",
      "=================================================================\n",
      "Total params: 2,028\n",
      "Trainable params: 2,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.2542,  loss:1.1377,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4205,  val_acc:0.2468,  val_loss:1.1185,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4448,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5149,  loss:0.9902,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777040.0000,  mean_squared_error:1.4427,  val_acc:0.5429,  val_loss:0.9692,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708960.0000,  val_mean_squared_error:1.4691,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5236,  loss:0.9797,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4447,  val_acc:0.5476,  val_loss:0.9613,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708968.0000,  val_mean_squared_error:1.4711,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5224,  loss:0.9765,  mean_absolute_error:1.0294,  mean_absolute_percentage_error:95777032.0000,  mean_squared_error:1.4478,  val_acc:0.5516,  val_loss:0.9576,  val_mean_absolute_error:1.0410,  val_mean_absolute_percentage_error:94708976.0000,  val_mean_squared_error:1.4752,  \n",
      "..................................................."
     ]
    }
   ],
   "source": [
    "# Jupyter notebook\n",
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128*8\n",
    "EPOCHS=10000\n",
    "validation_split = 0.2\n",
    "size_histories = {}\n",
    "\n",
    "##### Model01:\n",
    "\n",
    "model01_H1_H = tf.keras.Sequential([\n",
    "    layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "    layers.Dense(13, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H1_H = tf.keras.Sequential([\n",
    "    layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "    layers.Dense(21, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H1_H = tf.keras.Sequential([\n",
    "    layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "    layers.Dense(29, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H1_H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H1_H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01_H1_M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H1_M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H1_M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H1_M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H1_M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01_H1_L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H1_L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H1_L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H1_L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H1_L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "### 2 Hidden Layers:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01_H2_H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H2_H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H2_H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H2_H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H2_H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01_H2_M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H2_M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H2_M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H2_M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H2_M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01_H2_L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H2_L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H2_L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H2_L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H2_L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "### 3 Hidden Layers:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01_H3_H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H3_H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H3_H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H3_H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H3_H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01_H3_M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H3_M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H3_M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H3_M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H3_M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01_H3_L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H3_L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H3_L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H3_L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H3_L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Funnel Architecture:\n",
    "##### Model01:\n",
    "\n",
    "model01_H3_F = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H3_F = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(18, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H3_F = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(22, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H3_F = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(20, activation='relu'),\n",
    "  layers.Dense(15, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H3_F = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(19, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "### 4 Hidden Layers:\n",
    "#### High Amount of Neurons (H):\n",
    "##### Model01:\n",
    "\n",
    "model01_H4_H = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H4_H = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(21, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H4_H = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(29, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H4_H = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H4_H = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(33, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Medium Amount of Neurons (M):\n",
    "##### Model01:\n",
    "\n",
    "model01_H4_M = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H4_M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H4_M = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(14, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H4_M = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H4_M = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Low Amount of Neurons (L):\n",
    "##### Model01:\n",
    "\n",
    "model01_H4_L = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(4, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H4_L = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H4_L = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H4_L = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H4_L = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "#### Funnel Architecture:\n",
    "##### Model01:\n",
    "\n",
    "model01_H4_F = tf.keras.Sequential([\n",
    "  layers.Dense(13, activation='relu',input_shape=(train_X01.shape[1],)), # 13 features\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(10, activation='relu'),\n",
    "  layers.Dense(7, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model02:\n",
    "\n",
    "model02_H4_F = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(18, activation='relu'),\n",
    "  layers.Dense(13, activation='relu'),\n",
    "  layers.Dense(9, activation='relu'),\n",
    "  layers.Dense(5, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model03:\n",
    "\n",
    "model03_H4_F = tf.keras.Sequential([\n",
    "  layers.Dense(29, activation='relu',input_shape=(train_X03.shape[1],)), # 29 features\n",
    "  layers.Dense(22, activation='relu'),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model04:\n",
    "\n",
    "model04_H4_F = tf.keras.Sequential([\n",
    "  layers.Dense(25, activation='relu',input_shape=(train_X04.shape[1],)), # 25 features\n",
    "  layers.Dense(20, activation='relu'),\n",
    "  layers.Dense(15, activation='relu'),\n",
    "  layers.Dense(11, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "##### Model05:\n",
    "\n",
    "model05_H4_F = tf.keras.Sequential([\n",
    "  layers.Dense(33, activation='relu',input_shape=(train_X05.shape[1],)), # 33 features\n",
    "  layers.Dense(25, activation='relu'),\n",
    "  layers.Dense(19, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(6, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# H1_H\n",
    "size_histories['model01_H1_H'] = compile_and_fit(model01_H1_H, 'model01_H1_H', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H1_H'] = compile_and_fit(model02_H1_H, 'model02_H1_H', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H1_H'] = compile_and_fit(model03_H1_H, 'model03_H1_H', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H1_H'] = compile_and_fit(model04_H1_H, 'model04_H1_H', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H1_H'] = compile_and_fit(model05_H1_H, 'model05_H1_H', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H1_M\n",
    "size_histories['model01_H1_M'] = compile_and_fit(model01_H1_M, 'model01_H1_M', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H1_M'] = compile_and_fit(model02_H1_M, 'model02_H1_M', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H1_M'] = compile_and_fit(model03_H1_M, 'model03_H1_M', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H1_M'] = compile_and_fit(model04_H1_M, 'model04_H1_M', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H1_M'] = compile_and_fit(model05_H1_M, 'model05_H1_M', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H1_L\n",
    "size_histories['model01_H1_L'] = compile_and_fit(model01_H1_L, 'model01_H1_L', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H1_L'] = compile_and_fit(model02_H1_L, 'model02_H1_L', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H1_L'] = compile_and_fit(model03_H1_L, 'model03_H1_L', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H1_L'] = compile_and_fit(model04_H1_L, 'model04_H1_L', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H1_L'] = compile_and_fit(model05_H1_L, 'model05_H1_L', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H2_H\n",
    "size_histories['model01_H2_H'] = compile_and_fit(model01_H2_H, 'model01_H2_H', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H2_H'] = compile_and_fit(model02_H2_H, 'model02_H2_H', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H2_H'] = compile_and_fit(model03_H2_H, 'model03_H2_H', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H2_H'] = compile_and_fit(model04_H2_H, 'model04_H2_H', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H2_H'] = compile_and_fit(model05_H2_H, 'model05_H2_H', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H2_M\n",
    "size_histories['model01_H2_M'] = compile_and_fit(model01_H2_M, 'model01_H2_M', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H2_M'] = compile_and_fit(model02_H2_M, 'model02_H2_M', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H2_M'] = compile_and_fit(model03_H2_M, 'model03_H2_M', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H2_M'] = compile_and_fit(model04_H2_M, 'model04_H2_M', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H2_M'] = compile_and_fit(model05_H2_M, 'model05_H2_M', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H2_L\n",
    "size_histories['model01_H2_L'] = compile_and_fit(model01_H2_L, 'model01_H2_L', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H2_L'] = compile_and_fit(model02_H2_L, 'model02_H2_L', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H2_L'] = compile_and_fit(model03_H2_L, 'model03_H2_L', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H2_L'] = compile_and_fit(model04_H2_L, 'model04_H2_L', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H2_L'] = compile_and_fit(model05_H2_L, 'model05_H2_L', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H3_H\n",
    "size_histories['model01_H3_H'] = compile_and_fit(model01_H3_H, 'model01_H3_H', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H3_H'] = compile_and_fit(model02_H3_H, 'model02_H3_H', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H3_H'] = compile_and_fit(model03_H3_H, 'model03_H3_H', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H3_H'] = compile_and_fit(model04_H3_H, 'model04_H3_H', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H3_H'] = compile_and_fit(model05_H3_H, 'model05_H3_H', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H3_M\n",
    "size_histories['model01_H3_M'] = compile_and_fit(model01_H3_M, 'model01_H3_M', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H3_M'] = compile_and_fit(model02_H3_M, 'model02_H3_M', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H3_M'] = compile_and_fit(model03_H3_M, 'model03_H3_M', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H3_M'] = compile_and_fit(model04_H3_M, 'model04_H3_M', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H3_M'] = compile_and_fit(model05_H3_M, 'model05_H3_M', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H3_L\n",
    "size_histories['model01_H3_L'] = compile_and_fit(model01_H3_L, 'model01_H3_L', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H3_L'] = compile_and_fit(model02_H3_L, 'model02_H3_L', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H3_L'] = compile_and_fit(model03_H3_L, 'model03_H3_L', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H3_L'] = compile_and_fit(model04_H3_L, 'model04_H3_L', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H3_L'] = compile_and_fit(model05_H3_L, 'model05_H3_L', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H3_F\n",
    "size_histories['model01_H3_F'] = compile_and_fit(model01_H3_F, 'model01_H3_F', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H3_F'] = compile_and_fit(model02_H3_F, 'model02_H3_F', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H3_F'] = compile_and_fit(model03_H3_F, 'model03_H3_F', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H3_F'] = compile_and_fit(model04_H3_F, 'model04_H3_F', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H3_F'] = compile_and_fit(model05_H3_F, 'model05_H3_F', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H4_H\n",
    "size_histories['model01_H4_H'] = compile_and_fit(model01_H4_H, 'model01_H4_H', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H4_H'] = compile_and_fit(model02_H4_H, 'model02_H4_H', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H4_H'] = compile_and_fit(model03_H4_H, 'model03_H4_H', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H4_H'] = compile_and_fit(model04_H4_H, 'model04_H4_H', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H4_H'] = compile_and_fit(model05_H4_H, 'model05_H4_H', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H4_M\n",
    "size_histories['model01_H4_M'] = compile_and_fit(model01_H4_M, 'model01_H4_M', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H4_M'] = compile_and_fit(model02_H4_M, 'model02_H4_M', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H4_M'] = compile_and_fit(model03_H4_M, 'model03_H4_M', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H4_M'] = compile_and_fit(model04_H4_M, 'model04_H4_M', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H4_M'] = compile_and_fit(model05_H4_M, 'model05_H4_M', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H4_L\n",
    "size_histories['model01_H4_L'] = compile_and_fit(model01_H4_L, 'model01_H4_L', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H4_L'] = compile_and_fit(model02_H4_L, 'model02_H4_L', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H4_L'] = compile_and_fit(model03_H4_L, 'model03_H4_L', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H4_L'] = compile_and_fit(model04_H4_L, 'model04_H4_L', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H4_L'] = compile_and_fit(model05_H4_L, 'model05_H4_L', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "\n",
    "# H4_F\n",
    "size_histories['model01_H4_F'] = compile_and_fit(model01_H4_F, 'model01_H4_F', train_X01, train_y01, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model02_H4_F'] = compile_and_fit(model02_H4_F, 'model02_H4_F', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model03_H4_F'] = compile_and_fit(model03_H4_F, 'model03_H4_F', train_X03, train_y03, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model04_H4_F'] = compile_and_fit(model04_H4_F, 'model04_H4_F', train_X04, train_y04, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n",
    "size_histories['model05_H4_F'] = compile_and_fit(model05_H4_F, 'model05_H4_F', train_X05, train_y05, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "1DwuhGCJH2yL",
    "outputId": "13ba4f8a-0516-4937-d37a-8b0130fc5a56"
   },
   "outputs": [],
   "source": [
    "# H1_H\n",
    "score = load_model('../model/model01_H1_H.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model01_H1_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H1_H.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H1_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H1_H.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H1_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H1_H.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H1_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H1_H.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H1_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H1_M\n",
    "score = load_model('../model/model01_H1_M.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model01_H1_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H1_M.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H1_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H1_M.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H1_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H1_M.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H1_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H1_M.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H1_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H1_L\n",
    "score = load_model('../model/model01_H1_L.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model01_H1_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H1_L.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H1_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H1_L.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H1_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H1_L.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H1_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H1_L.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H1_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H2_H\n",
    "score = load_model('../model/model01_H2_H.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model01_H2_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H2_H.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H2_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H2_H.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H2_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H2_H.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H2_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H2_H.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H2_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H2_M\n",
    "score = load_model('../model/model01_H2_M.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model01_H2_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H2_M.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H2_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H2_M.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H2_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H2_M.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H2_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H2_M.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H2_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H2_L\n",
    "score = load_model('../model/model01_H2_L.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model01_H2_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H2_L.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H2_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H2_L.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H2_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H2_L.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H2_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H2_L.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H2_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H3_H\n",
    "score = load_model('../model/model01_H3_H.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H3_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H3_H.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H3_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H3_H.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H3_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H3_H.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H3_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H3_H.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H3_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H3_M\n",
    "score = load_model('../model/model01_H3_M.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H3_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H3_M.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H3_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H3_M.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H3_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H3_M.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H3_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H3_M.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H3_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H3_L\n",
    "score = load_model('../model/model01_H3_L.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H3_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H3_L.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H3_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H3_L.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H3_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H3_L.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H3_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H3_L.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H3_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H3_F\n",
    "score = load_model('../model/model01_H3_F.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H3_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H3_F.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H3_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H3_F.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H3_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H3_F.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H3_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H3_F.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H3_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H4_H\n",
    "score = load_model('../model/model01_H4_H.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H4_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H4_H.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H4_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H4_H.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H4_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H4_H.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H4_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H4_H.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H4_H\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H4_M\n",
    "score = load_model('../model/model01_H4_M.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H4_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H4_M.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H4_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H4_M.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H4_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H4_M.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H4_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H4_M.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H4_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H4_L\n",
    "score = load_model('../model/model01_H4_L.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H4_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H4_L.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H4_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H4_L.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H4_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H4_L.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H4_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H4_L.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H4_L\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "# H4_F\n",
    "score = load_model('../model/model01_H4_F.h5').evaluate(test_X01, test_y01, verbose=3)\n",
    "print(\"model02_H4_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model02_H4_F.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H4_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model03_H4_F.h5').evaluate(test_X03, test_y03, verbose=3)\n",
    "print(\"model03_H4_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model04_H4_F.h5').evaluate(test_X04, test_y04, verbose=3)\n",
    "print(\"model04_H4_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")\n",
    "\n",
    "score = load_model('../model/model05_H4_F.h5').evaluate(test_X05, test_y05, verbose=3)\n",
    "print(\"model05_H4_F\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])\n",
    "print(\"#####\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "colab-nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
