{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thu-soccer/project/blob/master/colab/colab_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "FtJth4hT577a",
    "outputId": "ce296e87-fcd1-4261-8057-6daacd747370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18740 train examples\n",
      "2083 test examples\n",
      "6329 train examples\n",
      "704 test examples\n",
      "6329 train examples\n",
      "704 test examples\n",
      "6296 train examples\n",
      "700 test examples\n",
      "6296 train examples\n",
      "700 test examples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 999)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_core.estimator import inputs\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots\n",
    "\n",
    "from  IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "\n",
    "def normalize_and_encode(dataframe):\n",
    "    column_names_to_not_normalize = ['result']\n",
    "    column_names_to_normalize = [x for x in list(dataframe) if x not in column_names_to_not_normalize ]\n",
    "    x = dataframe[column_names_to_normalize].values\n",
    "    x_scaled = preprocessing.normalize(x)\n",
    "    df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = dataframe.index)\n",
    "    dataframe[column_names_to_normalize] = df_temp\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit([ \"H\", \"A\", \"D\"])\n",
    "    dataframe.loc[:,['result']]=le.transform(dataframe['result'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def get_X_and_y(dataframe):\n",
    "    X = dataframe.drop(columns=['result']).values\n",
    "    y = dataframe[['result']].values\n",
    "    return X,y\n",
    "\n",
    "df01 = pd.read_csv('../data/sliding01.csv', sep=',', index_col=0)\n",
    "df02 = pd.read_csv('../data/sliding02_shots.csv', sep=',', index_col=0)\n",
    "df03 = pd.read_csv('../data/sliding03_shots_extra.csv', sep=',', index_col=0)\n",
    "df04 = pd.read_csv('../data/sliding04_shots_and_possession.csv', sep=',', index_col=0)\n",
    "df05 = pd.read_csv('../data/sliding05_shots_and_possession_extra.csv', sep=',', index_col=0)\n",
    "\n",
    "n01 = normalize_and_encode(df01)\n",
    "n02 = normalize_and_encode(df02)\n",
    "n03 = normalize_and_encode(df03)\n",
    "n04 = normalize_and_encode(df04)\n",
    "n05 = normalize_and_encode(df05)\n",
    "\n",
    "train01, test01 = train_test_split(n01, test_size=0.1)\n",
    "print(len(train01), 'train examples')\n",
    "print(len(test01), 'test examples')\n",
    "\n",
    "train02, test02 = train_test_split(n02, test_size=0.1)\n",
    "print(len(train02), 'train examples')\n",
    "print(len(test02), 'test examples')\n",
    "\n",
    "train03, test03 = train_test_split(n03, test_size=0.1)\n",
    "print(len(train03), 'train examples')\n",
    "print(len(test03), 'test examples')\n",
    "\n",
    "train04, test04 = train_test_split(n04, test_size=0.1)\n",
    "print(len(train04), 'train examples')\n",
    "print(len(test04), 'test examples')\n",
    "\n",
    "train05, test05 = train_test_split(n05, test_size=0.1)\n",
    "print(len(train04), 'train examples')\n",
    "print(len(test04), 'test examples')\n",
    "\n",
    "train_X01,train_y01 = get_X_and_y(train01)\n",
    "train_X02,train_y02 = get_X_and_y(train02)\n",
    "train_X03,train_y03 = get_X_and_y(train03)\n",
    "train_X04,train_y04 = get_X_and_y(train04)\n",
    "train_X05,train_y05 = get_X_and_y(train05)\n",
    "\n",
    "test_X01,test_y01 = get_X_and_y(test01)\n",
    "test_X02,test_y02 = get_X_and_y(test02)\n",
    "test_X03,test_y03 = get_X_and_y(test03)\n",
    "test_X04,test_y04 = get_X_and_y(test04)\n",
    "test_X05,test_y05 = get_X_and_y(test05)\n",
    "\n",
    "\n",
    "#Many models train better if you gradually reduce the learning rate during training. Use optimizers.schedules to reduce the learning rate over time:\n",
    "#The code sets a schedules.InverseTimeDecay to hyperbolically decrease the learning rate to 1/2 of the base rate at 1000 epochs, 1/3 at 2000 epochs and so on.\n",
    "\n",
    "def get_lr_schedule(train, batch_size):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=(len(train)//batch_size)*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False)\n",
    "    return lr_schedule\n",
    "\n",
    "def get_optimizer(train, batch_size):\n",
    "    return tf.keras.optimizers.Adam(get_lr_schedule(train, batch_size))\n",
    "\n",
    "\n",
    "#Each model in this tutorial will use the same training configuration. So set these up in a reusable way, starting with the list of callbacks.\n",
    "#The training for this tutorial runs for many short epochs. To reduce the logging noise use the tfdocs.EpochDots which simply a . for each epoch and, and a full set of metrics every 100 epochs.\n",
    "\n",
    "def get_callbacks(name):\n",
    "    return [\n",
    "        tfdocs.modeling.EpochDots(),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200),\n",
    "        #tf.keras.callbacks.TensorBoard(logdir/name), # Jupyter Notebook\n",
    "        #tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1) # Google Colab\n",
    "      ]\n",
    "\n",
    "def compile_and_fit(model, name, X, y, validation_split, batch_size, optimizer=None, max_epochs=1000):\n",
    "    if optimizer is None:\n",
    "        optimizer = get_optimizer(X, batch_size)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy','mse', 'mae', 'mape'])\n",
    "\n",
    "    model.summary()\n",
    "     \n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_split=validation_split,\n",
    "        batch_size=batch_size,\n",
    "#        steps_per_epoch = 50, # (len(train_X01)//batch_size,\n",
    "        epochs=max_epochs,\n",
    "        callbacks=get_callbacks(name),\n",
    "        verbose=0)\n",
    "    \n",
    "    model.save(\"../model/%s.h5\" %name) \n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_history(model_history):\n",
    "\tplt.plot(model_history.history['accuracy'])\n",
    "\tplt.plot(model_history.history['val_accuracy'])\n",
    "\tplt.title(\"%s accuracy\" %model_history)\n",
    "\tplt.ylabel('accuracy')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n",
    "\t\n",
    "\tplt.plot(model_history.history['loss'])\n",
    "\tplt.plot(model_history.history['val_loss'])\n",
    "\tplt.title(\"%s loss\" %model_history)\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.legend(['train', 'val'], loc='upper left')\n",
    "\tplt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HtHfSEgO6sUN",
    "outputId": "b7fd6a48-30f1-470e-f92b-118a7046a09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 12)                264       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 1,077\n",
      "Trainable params: 1,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch: 0, acc:0.3761,  loss:1.0935,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4244,  val_acc:0.4518,  val_loss:1.0921,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4091,  \n",
      "....................................................................................................\n",
      "Epoch: 100, acc:0.5301,  loss:0.9753,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4507,  val_acc:0.5205,  val_loss:0.9938,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4362,  \n",
      "....................................................................................................\n",
      "Epoch: 200, acc:0.5360,  loss:0.9679,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4552,  val_acc:0.5229,  val_loss:0.9880,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627152.0000,  val_mean_squared_error:1.4384,  \n",
      "....................................................................................................\n",
      "Epoch: 300, acc:0.5406,  loss:0.9636,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558128.0000,  mean_squared_error:1.4548,  val_acc:0.5245,  val_loss:0.9844,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4382,  \n",
      "....................................................................................................\n",
      "Epoch: 400, acc:0.5400,  loss:0.9618,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4563,  val_acc:0.5269,  val_loss:0.9808,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627152.0000,  val_mean_squared_error:1.4398,  \n",
      "....................................................................................................\n",
      "Epoch: 500, acc:0.5445,  loss:0.9596,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4555,  val_acc:0.5316,  val_loss:0.9778,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627144.0000,  val_mean_squared_error:1.4395,  \n",
      "....................................................................................................\n",
      "Epoch: 600, acc:0.5443,  loss:0.9588,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558128.0000,  mean_squared_error:1.4558,  val_acc:0.5316,  val_loss:0.9770,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627168.0000,  val_mean_squared_error:1.4403,  \n",
      "....................................................................................................\n",
      "Epoch: 700, acc:0.5443,  loss:0.9585,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4558,  val_acc:0.5324,  val_loss:0.9765,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627144.0000,  val_mean_squared_error:1.4408,  \n",
      "....................................................................................................\n",
      "Epoch: 800, acc:0.5451,  loss:0.9581,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4565,  val_acc:0.5316,  val_loss:0.9758,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4406,  \n",
      "....................................................................................................\n",
      "Epoch: 900, acc:0.5455,  loss:0.9585,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558128.0000,  mean_squared_error:1.4562,  val_acc:0.5340,  val_loss:0.9756,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4407,  \n",
      "....................................................................................................\n",
      "Epoch: 1000, acc:0.5449,  loss:0.9581,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4561,  val_acc:0.5348,  val_loss:0.9750,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4402,  \n",
      "....................................................................................................\n",
      "Epoch: 1100, acc:0.5445,  loss:0.9581,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4571,  val_acc:0.5332,  val_loss:0.9752,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627144.0000,  val_mean_squared_error:1.4407,  \n",
      "....................................................................................................\n",
      "Epoch: 1200, acc:0.5455,  loss:0.9581,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558128.0000,  mean_squared_error:1.4568,  val_acc:0.5371,  val_loss:0.9747,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4395,  \n",
      "....................................................................................................\n",
      "Epoch: 1300, acc:0.5445,  loss:0.9575,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4569,  val_acc:0.5363,  val_loss:0.9746,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4403,  \n",
      "....................................................................................................\n",
      "Epoch: 1400, acc:0.5443,  loss:0.9575,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4568,  val_acc:0.5308,  val_loss:0.9749,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4406,  \n",
      "....................................................................................................\n",
      "Epoch: 1500, acc:0.5445,  loss:0.9577,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4566,  val_acc:0.5363,  val_loss:0.9745,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627168.0000,  val_mean_squared_error:1.4396,  \n",
      "....................................................................................................\n",
      "Epoch: 1600, acc:0.5439,  loss:0.9576,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4563,  val_acc:0.5363,  val_loss:0.9745,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627144.0000,  val_mean_squared_error:1.4399,  \n",
      "....................................................................................................\n",
      "Epoch: 1700, acc:0.5461,  loss:0.9572,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4569,  val_acc:0.5324,  val_loss:0.9751,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627160.0000,  val_mean_squared_error:1.4412,  \n",
      "....................................................................................................\n",
      "Epoch: 1800, acc:0.5436,  loss:0.9572,  mean_absolute_error:1.0303,  mean_absolute_percentage_error:98558120.0000,  mean_squared_error:1.4565,  val_acc:0.5332,  val_loss:0.9746,  val_mean_absolute_error:1.0276,  val_mean_absolute_percentage_error:91627152.0000,  val_mean_squared_error:1.4405,  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................."
     ]
    }
   ],
   "source": [
    "# Jupyter notebook\n",
    "logdir = pathlib.Path(tempfile.mkdtemp())/\"tensorboard_logs\"\n",
    "shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128*8\n",
    "EPOCHS=10000\n",
    "validation_split = 0.2\n",
    "size_histories = {}\n",
    "\n",
    "model02_H3_M = tf.keras.Sequential([\n",
    "  layers.Dense(21, activation='relu',input_shape=(train_X02.shape[1],)), # 21 features\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(12, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "size_histories['model02_H3_M'] = compile_and_fit(model02_H3_M, 'model02_H3_M', train_X02, train_y02, validation_split=validation_split,batch_size=BATCH_SIZE,max_epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "1DwuhGCJH2yL",
    "outputId": "13ba4f8a-0516-4937-d37a-8b0130fc5a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model02_H3_M\n",
      "Loss: 0.9788458130576394\n",
      "Test Accuracy: 0.51704544\n",
      "Mean Square Error: 1.4649594\n",
      "Mean Absolute Error: 1.0383523\n",
      "Mean Absolute Percentage Error: 89962170.0\n"
     ]
    }
   ],
   "source": [
    "score = load_model('../model/model02_H3_M.h5').evaluate(test_X02, test_y02, verbose=3)\n",
    "print(\"model02_H3_M\")\n",
    "print(\"Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])\n",
    "print(\"Mean Square Error:\",score[2])\n",
    "print(\"Mean Absolute Error:\",score[3])\n",
    "print(\"Mean Absolute Percentage Error:\",score[4])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "colab-nn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
